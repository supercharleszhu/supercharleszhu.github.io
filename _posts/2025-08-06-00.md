---
title: Torch.compile Deep Dive - From Python Function to Triton Kernel
date: 2025-08-07 10:00:00 +0800
categories: [Machine Learning, PyTorch]
tags: [pytorch, torch.compile, triton, optimization, deep-learning]
author: Charles Zhu
toc: true
description: An interactive deep dive into PyTorch's torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.
---

# Torch.compile Deep Dive: From Python Function to Triton Kernel

## Introduction

PyTorch's `torch.compile` represents a revolutionary approach to optimizing PyTorch code. In this interactive deep dive, we'll trace the journey of a simple Python function through the entire compilation pipeline, from the initial function definition to the final execution of optimized Triton kernels.

**🎯 Interactive Goal**: By the end of this guide, you'll be able to run commands and see the actual compilation process in action!

---

## 🚀 Quick Start: Run the Complete Demo

First, let's run a complete demonstration to see the entire pipeline in action:

```bash
# Clone or download the demo files, then run:
chmod +x run_torch_compile_demo.sh
./run_torch_compile_demo.sh
```

This will show you the complete pipeline with performance comparisons. Now let's break down each phase step by step.

---

## Phase 1: TorchDynamo - Graph Capture

### What is TorchDynamo?

TorchDynamo is PyTorch's graph capture mechanism that converts Python functions into computational graphs. It is hacking into the CPython's compilation stage ([reference](https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html#pep-523-adding-a-frame-evaluation-api-to-cpython) and modify the bytecode to be actually executed.

![TorchDynamo Architecture](/assets/img/posts/dynamo.png)

### 🔬 Deep Dive: TorchDynamo's Internal Architecture

Let's dive deep into code base and check how it is tracing the program!

#### **Entry Point**
An execution of a Python program roughly consists of three stages:

1. Initialization
2. Compilation
3. lInterpretation

The dynamo analysis was actually hacking into compilation stage, and rewrite the bytecode to be executed. When there is no already compiled function, we start a fresh analysis and tracing from scratch.

First step, it is using `dis` module to get the bytecode to be executed by CPython and run some initial cleanup of the Bytecode. You can check compiler [here](https://github.com/python/cpython/blob/main/InternalDocs/compiler.md) on details of how it is generating the bytecode. Also [here](https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/) is a great blog sharing how CPython is compiling and interpreting a python program

```python
# torch version: 2.5.1+cu126

# From bytecode_transformation.py
def cleaned_instructions(code, safe=False) -> List[Instruction]:
    instructions = list(map(convert_instruction, dis.get_instructions(code)))
    # rest part are omitted
```

#### **Symbolic Execution Engine**

The core tracing happens in `symbolic_convert.py` through the `InstructionTranslator` class:

```python
# From symbolic_convert.py
class InstructionTranslatorBase:
    def step(self):
        """Process exactly one instruction, return False we should exit"""
        ip = self.instruction_pointer
        if ip is None:
            return False
        self.current_instruction = inst = self.instructions[ip]
        self.instruction_pointer = ip + 1
        
        # Dispatch to appropriate handler
        self.dispatch_table[inst.opcode](self, inst)
```

**Instruction-by-Instruction Processing:**
- Each Python bytecode instruction has a corresponding handler
- Variables are tracked as `VariableTracker` objects
- Stack management maintains symbolic state

#### **4. Variable Tracking System**

TorchDynamo uses a sophisticated variable tracking hierarchy:

```python
# From variables/base.py
class VariableTracker:
    def call_function(self, tx, args, kwargs) -> "VariableTracker":
        # Default implementation - can be overridden by subclasses
        
    def call_method(self, tx, name, args, kwargs) -> "VariableTracker":
        # Handle method calls on tracked objects
```

**Key Variable Types:**
- `TensorVariable`: Represents PyTorch tensors
- `ConstantVariable`: Represents Python constants  
- `ListVariable`, `TupleVariable`: Represents sequences
- `NNModuleVariable`: Represents neural network modules
- `BuiltinVariable`: Represents Python builtins

#### **5. Bytecode Instruction Handlers**

Each Python bytecode instruction has a specialized handler:

```python
# From symbolic_convert.py
def LOAD_FAST(self, inst):
    """Handle loading local variables"""
    self._load_fast(inst.argval)

def STORE_FAST(self, inst):
    """Handle storing local variables"""
    self._store_fast(inst.argval)

def CALL_FUNCTION(self, inst):
    """Handle function calls"""
    args = self.popn(inst.argval)
    fn = self.pop()
    self.call_function(fn, args, {})

def BINARY_ADD(self, inst):
    """Handle addition operations"""
    b, a = self.popn(2)
    self.push(a.call_method(self, "__add__", [b], {}))
```

#### **6. Graph Construction via FX**

The `OutputGraph` class manages FX graph construction:

```python
# From output_graph.py
class OutputGraph:
    def create_proxy(self, *args, **kwargs):
        # Creates FX proxy objects for graph nodes
        
    def compile_subgraph(self, tx, partial_convert=False, reason=None):
        # Compiles the current subgraph into an FX GraphModule
```

**Key Components:**
- **FX Graph**: Built incrementally as instructions are processed
- **Proxy Objects**: Represent symbolic computations
- **Guards**: Generated to ensure graph validity across different inputs

#### **7. Graph Break Handling**

When TorchDynamo encounters unsupported operations, it creates graph breaks:

```python
# From symbolic_convert.py
def break_graph_if_unsupported(*, push):
    def decorator(inner_fn):
        def wrapper(self, inst):
            try:
                return inner_fn(self, inst)
            except Unsupported:
                # Create graph break and restart analysis
                self.current_speculation.fail_and_restart_analysis()
```

#### **8. Speculation and Restart System**

TorchDynamo uses a sophisticated speculation system:

```python
# From symbolic_convert.py
class SpeculationLog:
    def restart(self):
        # Restart the dynamo conversion process from the beginning
        # When hitting the start of speculation that failed, generate a graph break
```

**Speculation Process:**
1. **Checkpoint Creation**: Save current state
2. **Optimistic Execution**: Try to continue with current assumptions
3. **Failure Detection**: If assumptions fail, restart from checkpoint
4. **Graph Break Generation**: Insert break points where needed

#### **9. Guard Generation**

TorchDynamo generates comprehensive guards to ensure correctness:

```python
# From guards.py
def install_guard(self, guard_fn):
    # Install guards for tensor shapes, types, and values
    # Guards ensure graph validity across different inputs
```

**Guard Types:**
- **Shape Guards**: Ensure tensor dimensions match
- **Type Guards**: Ensure tensor dtypes are consistent
- **Value Guards**: Ensure constant values match
- **Global Guards**: Track changes to global state

#### **Post Processing**

Instead of directly letting CPython executing the bytecode, TorchDynamo runs a tracer via 

```python
# From convert_frame.py
def transform(instructions: List[Instruction], code_options: Dict[str, object]) -> None:
    # Remove dead code and pointless jumps
    instructions[:] = remove_dead_code(instructions)
    instructions[:] = remove_pointless_jumps(instructions)
    
    # Process exception tables for Python 3.11+
    propagate_inst_exn_table_entries(instructions)
```

**Key Transformations:**
- **Dead Code Elimination**: Removes unreachable instructions
- **Jump Optimization**: Simplifies control flow
- **Exception Table Processing**: Handles try/except blocks for Python 3.11+

### Interactive Example: Let's Trace a Function

Create a file called `trace_example.py`:

```python
import torch
import os

# Set environment variables for detailed debugging
os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo,+inductor'

def simple_function(x, y):
    """A simple function to trace through TorchDynamo"""
    print("Executing function...")
    a = x + y
    b = a * 2.0
    c = torch.relu(b)
    return c

# Create tensors
input1 = torch.randn(100, 100).to(device="cuda:0")
input2 = torch.randn(100, 100).to(device="cuda:0")

print("=== PHASE 1: TorchDynamo Graph Capture ===")
print("Calling torch.compile triggers TorchDynamo...")

# This is where the magic happens!
compiled_fn = torch.compile(simple_function, backend="inductor")

print("First execution triggers compilation...")
result = compiled_fn(input1, input2)
print(f"Result shape: {result.shape}")
```

**Run this example:**
```bash
python trace_example.py
```

**What you'll see:**
- Detailed bytecode tracing output showing each instruction
- FX graph construction with node creation
- Guard generation for tensor shapes and types
- Variable tracking and symbolic execution

### Understanding the Output

When you run the above, you'll see output like this:

```
V0801 22:21:47.817000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:864] [1/0] torchdynamo start compiling
V0801 22:21:47.818000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:865] [1/0] [__trace_source] TRACE starts_line
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE LOAD_FAST x []
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE LOAD_FAST y [LazyVariableTracker()]
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE BINARY_ADD None [LazyVariableTracker(), LazyVariableTracker()]
```

This shows TorchDynamo analyzing each bytecode instruction and building the computational graph.

### 🔍 Advanced: Examining the Internal Process

To see the detailed internal process, create `advanced_trace.py`:

```python
import torch
import os
import dis

# Set environment variables for maximum debugging
os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo,+inductor'
os.environ['TORCH_LOGS_LEVEL'] = 'DEBUG'

def advanced_function(x, y):
    """Function to demonstrate advanced TorchDynamo features"""
    # Show bytecode analysis
    print("=== Bytecode Analysis ===")
    dis.dis(advanced_function)
    
    # Multiple operations to trace
    a = x + y
    b = a * 2.0
    c = torch.relu(b)
    d = c.sum(dim=1)
    e = d.mean()
    return e

# Create tensors
input1 = torch.randn(100, 100).to(device="cuda:0")
input2 = torch.randn(100, 100).to(device="cuda:0")

print("=== ADVANCED TORCHDYNAMO ANALYSIS ===")
print("This will show detailed internal process...")

compiled_fn = torch.compile(advanced_function, backend="inductor")

print("First execution with detailed tracing...")
result = compiled_fn(input1, input2)
print(f"Result: {result}")

print("\n=== INTERNAL PROCESS SUMMARY ===")
print("1. Bytecode Analysis: Function bytecode was analyzed")
print("2. Symbolic Execution: Variables tracked as VariableTracker objects")
print("3. Graph Construction: FX graph built incrementally")
print("4. Guard Generation: Guards created for tensor shapes/types")
print("5. Graph Break Handling: Unsupported operations handled gracefully")
```

**Run the advanced example:**
```bash
python advanced_trace.py
```

### 🎯 Key TorchDynamo Concepts

**1. Symbolic Execution:**
- Variables are tracked as `VariableTracker` objects, not concrete values
- Operations are recorded symbolically in the FX graph
- This allows for optimization and fusion opportunities

**2. Graph Breaks:**
- When TorchDynamo encounters unsupported operations, it creates graph breaks
- The function is split into multiple graphs at break points
- This ensures compatibility while still providing optimization benefits

**3. Guard Generation:**
- Guards ensure the compiled graph is valid for different inputs
- Guards check tensor shapes, types, and other properties
- If guards fail, the graph is recompiled with new assumptions

**4. Speculation:**
- TorchDynamo makes optimistic assumptions about code behavior
- If assumptions fail, it restarts analysis with more conservative approach
- This balances performance with correctness

This sophisticated architecture allows TorchDynamo to efficiently trace Python code, handle complex Python constructs, and generate optimized computational graphs while maintaining correctness through comprehensive guard generation.

---

## Phase 2: TorchInductor - Graph Optimization

### What is TorchInductor?

TorchInductor is PyTorch's backend compiler that takes the FX graph from TorchDynamo and optimizes it for execution on various backends (CPU, CUDA, etc.). For CUDA, it generates Triton kernels.

### Interactive Example: See the Optimization Process

Create a file called `inductor_example.py`:

```python
import torch
import os

# Set environment variables for detailed debugging
os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo,+inductor'

def complex_function(x, y):
    """A function that will trigger various optimizations"""
    # Multiple operations that can be fused
    a = x + y
    b = a * 2.0
    c = torch.relu(b)
    
    # Reduction operations that trigger Triton code generation
    d = c.sum(dim=1)  # Reduce along dimension 1
    e = d.mean()       # Global mean
    
    return e

# Create larger tensors to see more optimization
input1 = torch.randn(1000, 1000).to(device="cuda:0")
input2 = torch.randn(1000, 1000).to(device="cuda:0")

print("=== PHASE 2: TorchInductor Optimization ===")
print("This will show graph optimization and fusion...")

compiled_fn = torch.compile(complex_function, backend="inductor")

print("First execution triggers Inductor optimization...")
result = compiled_fn(input1, input2)
print(f"Result: {result}")
```

**Run this example:**
```bash
python inductor_example.py
```

**What you'll see:**
- Graph lowering and buffer analysis
- Scheduler optimization
- Fusion analysis and kernel generation

### Check Generated Files

After running the example, check the debug directory:

```bash
# Find the latest debug directory
find torch_compile_debug/run_*/torchinductor/*/ -name "*.py" -o -name "*.txt" | head -10
```

This will show you the generated files including:
- `output_code.py`: Generated Triton kernels
- `ir_pre_fusion.txt`: IR before fusion
- `ir_post_fusion.txt`: IR after fusion

---

## Phase 3: Triton Kernel Generation

### What is Triton?

Triton is a language and compiler for writing highly optimized GPU kernels. It provides:
- **High-level abstractions** for GPU programming
- **Automatic optimization** of memory access patterns
- **Parallel execution** primitives
- **Integration** with PyTorch

### Interactive Example: Force Triton Code Generation

Create a file called `triton_example.py`:

```python
import torch
import os

# Set environment variables for detailed debugging
os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo,+inductor'

def triton_trigger_function(x, y):
    """
    This function is designed to trigger Triton code generation.
    Uses operations that benefit from custom kernels.
    """
    # Multiple operations that can be fused into a single kernel
    a = x + y
    b = a * 2.0
    c = torch.relu(b)
    
    # Reduction operations that definitely trigger Triton
    d = c.sum(dim=1)  # Reduce along dimension 1
    e = d.mean()       # Global mean
    
    # More complex operations
    f = torch.sigmoid(e)
    g = f * 10.0
    
    return g

# Create tensors with specific shapes to trigger Triton compilation
input1 = torch.randn(2000, 2000).to(device="cuda:0")
input2 = torch.randn(2000, 2000).to(device="cuda:0")

print("=== PHASE 3: Triton Kernel Generation ===")
print("This should generate actual Triton kernels...")

compiled_fn = torch.compile(triton_trigger_function, backend="inductor")

print("First execution triggers Triton kernel generation...")
result = compiled_fn(input1, input2)
print(f"Result: {result}")

print("\n" + "="*50)
print("Triton code should have been generated!")
print("Check the debug directory for generated files.")
print("="*50)
```

**Run this example:**
```bash
python triton_example.py
```

**Check for Triton kernels:**
```bash
# Look for output_code.py files
find torch_compile_debug/run_*/torchinductor/*/ -name "output_code.py" -exec ls -la {} \;
```

**Examine the generated Triton code:**
```bash
# View the generated Triton kernel
find torch_compile_debug/run_*/torchinductor/*/ -name "output_code.py" -exec head -20 {} \;
```

---

## Phase 4: Runtime Execution

### Interactive Performance Comparison

Create a file called `performance_comparison.py`:

```python
import torch
import time
import os

# Set environment variables for detailed debugging
os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo,+inductor'

def benchmark_function(x, y):
    """Function to benchmark"""
    a = x + y
    b = a * 2.0
    c = torch.relu(b)
    d = c.sum(dim=1)
    e = d.mean()
    return e

# Create tensors
input1 = torch.randn(1000, 1000).to(device="cuda:0")
input2 = torch.randn(1000, 1000).to(device="cuda:0")

print("=== PHASE 4: Runtime Performance Comparison ===")

# Benchmark original function
print("Benchmarking original function...")
torch.cuda.synchronize()
start_time = time.time()

for _ in range(10):
    result_original = benchmark_function(input1, input2)

torch.cuda.synchronize()
original_time = time.time() - start_time
print(f"Original function time: {original_time:.6f} seconds")

# Benchmark compiled function
print("\nBenchmarking compiled function...")
compiled_fn = torch.compile(benchmark_function, backend="inductor")

# First run (includes compilation time)
torch.cuda.synchronize()
start_time = time.time()
result_compiled = compiled_fn(input1, input2)
torch.cuda.synchronize()
first_run_time = time.time() - start_time
print(f"First run (with compilation): {first_run_time:.6f} seconds")

# Subsequent runs (optimized execution)
torch.cuda.synchronize()
start_time = time.time()

for _ in range(10):
    result_compiled = compiled_fn(input1, input2)

torch.cuda.synchronize()
compiled_time = time.time() - start_time
print(f"Subsequent runs: {compiled_time:.6f} seconds")

# Performance analysis
speedup = original_time / (compiled_time / 10)
improvement = ((original_time - (compiled_time / 10)) / original_time) * 100

print(f"\n{'='*50}")
print("PERFORMANCE ANALYSIS")
print(f"{'='*50}")
print(f"Original execution time: {original_time:.6f} seconds")
print(f"Compiled execution time: {compiled_time/10:.6f} seconds")
print(f"Speedup: {speedup:.2f}x")
print(f"Performance improvement: {improvement:.1f}%")

if speedup > 1:
    print("✅ Compilation provided performance benefits!")
else:
    print("⚠️  Compilation did not provide performance benefits in this case.")
```

**Run this example:**
```bash
python performance_comparison.py
```

---

## 🔍 Debugging and Inspection

### Interactive Debug Commands

Here are some useful commands to explore the compilation process:

**1. View all generated files:**
```bash
find torch_compile_debug/run_*/torchinductor/*/ -type f -name "*.py" -o -name "*.txt" | sort
```

**2. Examine the FX graph:**
```bash
find torch_compile_debug/run_*/torchinductor/*/ -name "fx_graph_*.py" -exec head -30 {} \;
```

**3. Check for Triton kernels:**
```bash
find torch_compile_debug/run_*/torchinductor/*/ -name "output_code.py" -exec wc -l {} \;
```

**4. View IR transformations:**
```bash
find torch_compile_debug/run_*/torchinductor/*/ -name "ir_*.txt" -exec head -20 {} \;
```

### Interactive Debug Script

Create a file called `debug_explorer.py`:

```python
#!/usr/bin/env python3
"""
Interactive debug explorer for PyTorch compile
"""

import os
import glob
import subprocess

def find_latest_debug_dir():
    """Find the latest debug directory"""
    debug_dirs = glob.glob("torch_compile_debug/run_*/torchinductor/*/")
    if not debug_dirs:
        return None
    return max(debug_dirs, key=os.path.getctime)

def explore_debug_files():
    """Explore the generated debug files"""
    debug_dir = find_latest_debug_dir()
    if not debug_dir:
        print("❌ No debug directories found")
        return
    
    print(f"🔍 Exploring debug directory: {debug_dir}")
    print("="*60)
    
    # List all files
    files = glob.glob(os.path.join(debug_dir, "*"))
    print("📁 Generated files:")
    for file in sorted(files):
        if os.path.isfile(file):
            size = os.path.getsize(file)
            print(f"  {os.path.basename(file)} ({size} bytes)")
    
    print("\n" + "="*60)
    
    # Check for Triton kernels
    output_code_files = glob.glob(os.path.join(debug_dir, "output_code.py"))
    if output_code_files:
        print("✅ Triton kernels generated!")
        for file in output_code_files:
            print(f"  📄 {file}")
            
        # Show snippet of Triton code
        print("\n📝 Triton kernel snippet:")
        with open(output_code_files[0], 'r') as f:
            lines = f.readlines()
            for i, line in enumerate(lines[:10]):
                print(f"  {i+1:2d}: {line.rstrip()}")
            if len(lines) > 10:
                print("  ...")
    else:
        print("❌ No Triton kernels generated")
    
    # Check for IR files
    ir_files = glob.glob(os.path.join(debug_dir, "ir_*.txt"))
    if ir_files:
        print(f"\n📊 IR files found: {len(ir_files)}")
        for file in ir_files:
            print(f"  📄 {os.path.basename(file)}")

if __name__ == "__main__":
    print("🔍 PyTorch Compile Debug Explorer")
    print("="*60)
    explore_debug_files()
```

**Run the debug explorer:**
```bash
python debug_explorer.py
```

---

## 🎯 Complete Interactive Demo

### One-Command Demo

Create a file called `complete_demo.py`:

```python
#!/usr/bin/env python3
"""
Complete interactive demo of PyTorch compile pipeline
"""

import torch
import time
import os
import glob

# Set environment variables
os.environ['TORCH_COMPILE_DEBUG'] = '1'
os.environ['TORCH_LOGS'] = '+dynamo,+inductor'

def demo_function(x, y):
    """Function for the complete demo"""
    a = x + y
    b = a * 2.0
    c = torch.relu(b)
    d = c.sum(dim=1)
    e = d.mean()
    return e

def run_complete_demo():
    """Run the complete interactive demo"""
    print("🚀 PyTorch Compile Complete Interactive Demo")
    print("="*60)
    
    # Check CUDA availability
    if not torch.cuda.is_available():
        print("❌ CUDA not available. This demo requires CUDA.")
        return
    
    print(f"✅ CUDA available: {torch.cuda.get_device_name()}")
    print(f"📦 PyTorch version: {torch.__version__}")
    
    # Create tensors
    print("\n📦 Creating input tensors...")
    input1 = torch.randn(1000, 1000).to(device="cuda:0")
    input2 = torch.randn(1000, 1000).to(device="cuda:0")
    print(f"   Input shapes: {input1.shape}, {input2.shape}")
    
    # Phase 1: TorchDynamo
    print("\n🔄 PHASE 1: TorchDynamo Graph Capture")
    print("   Calling torch.compile triggers TorchDynamo...")
    
    compiled_fn = torch.compile(demo_function, backend="inductor")
    
    # Phase 2-3: TorchInductor + Triton
    print("\n⚡ PHASE 2-3: TorchInductor Optimization + Triton Generation")
    print("   First execution triggers compilation...")
    
    torch.cuda.synchronize()
    start_time = time.time()
    result = compiled_fn(input1, input2)
    torch.cuda.synchronize()
    compilation_time = time.time() - start_time
    
    print(f"   Compilation + first execution time: {compilation_time:.6f} seconds")
    print(f"   Result: {result}")
    
    # Phase 4: Performance comparison
    print("\n🏃 PHASE 4: Performance Comparison")
    
    # Benchmark original
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(10):
        _ = demo_function(input1, input2)
    torch.cuda.synchronize()
    original_time = time.time() - start_time
    
    # Benchmark compiled
    torch.cuda.synchronize()
    start_time = time.time()
    for _ in range(10):
        _ = compiled_fn(input1, input2)
    torch.cuda.synchronize()
    compiled_time = time.time() - start_time
    
    # Analysis
    speedup = original_time / compiled_time
    improvement = ((original_time - compiled_time) / original_time) * 100
    
    print(f"   Original execution time: {original_time:.6f} seconds")
    print(f"   Compiled execution time: {compiled_time:.6f} seconds")
    print(f"   Speedup: {speedup:.2f}x")
    print(f"   Performance improvement: {improvement:.1f}%")
    
    if speedup > 1:
        print("   ✅ Compilation provided performance benefits!")
    else:
        print("   ⚠️  Compilation did not provide performance benefits.")
    
    # Debug information
    print("\n🔍 DEBUG INFORMATION")
    debug_dirs = glob.glob("torch_compile_debug/run_*/torchinductor/*/")
    if debug_dirs:
        latest_dir = max(debug_dirs, key=os.path.getctime)
        print(f"   Latest debug directory: {latest_dir}")
        
        # Check for Triton kernels
        output_code_files = glob.glob(os.path.join(latest_dir, "output_code.py"))
        if output_code_files:
            print("   ✅ Triton kernels generated!")
        else:
            print("   ❌ No Triton kernels generated")
    else:
        print("   ❌ No debug directories found")
    
    print("\n" + "="*60)
    print("🎉 Demo completed! Check the generated debug files.")
    print("="*60)

if __name__ == "__main__":
    run_complete_demo()
```

**Run the complete demo:**
```bash
python complete_demo.py
```

---

## 🎓 Key Takeaways

### What You've Learned

1. **TorchDynamo**: Converts Python functions to computational graphs through sophisticated bytecode analysis and symbolic execution
2. **TorchInductor**: Optimizes graphs and generates backend-specific code
3. **Triton**: Creates highly optimized GPU kernels
4. **Performance**: Significant speedups through kernel fusion and memory optimization

### Interactive Commands Summary

```bash
# Run the complete demo
python complete_demo.py

# Explore debug files
python debug_explorer.py

# Check for Triton kernels
find torch_compile_debug/run_*/torchinductor/*/ -name "output_code.py"

# View generated files
find torch_compile_debug/run_*/torchinductor/*/ -type f | sort
```

### Next Steps

1. **Experiment with different functions**: Try different operations to see what triggers Triton generation
2. **Explore the debug files**: Examine the generated code to understand the optimizations
3. **Benchmark your own code**: Apply `torch.compile` to your own PyTorch functions
4. **Read the documentation**: Dive deeper into PyTorch's compilation system

The combination of these technologies makes PyTorch's compilation system one of the most sophisticated and effective approaches to optimizing deep learning workloads. Happy compiling! 🚀 