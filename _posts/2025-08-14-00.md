---
title: From FlashAttention to FlashAttention 2
date: 2025-08-14 10:00:00 +0800
categories: [Machine Learning, PyTorch]
tags: [pytorch, torch.compile, triton, optimization, deep-learning]
author: Charles Zhu
toc: true
description: Deep dive into FlashAttention 2
---

## Introduction

This will be a codelevel walkthough on how the flashattention 2 was implemented, and why it is better than FlashAttention 1

## Background: FlashAttention

FlashAttention is a fused attention kernel that is minimizing the I/O overhead to access HBM memory. 