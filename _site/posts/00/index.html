<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Torch.compile 101 - From Python Function to Triton Kernel" />
<meta name="author" content="Charles Zhu" />
<meta property="og:locale" content="en" />
<meta name="description" content="An interactive deep dive into PyTorch’s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis." />
<meta property="og:description" content="An interactive deep dive into PyTorch’s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis." />
<link rel="canonical" href="http://localhost:4000/posts/00/" />
<meta property="og:url" content="http://localhost:4000/posts/00/" />
<meta property="og:site_name" content="ZCache" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-06T19:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Torch.compile 101 - From Python Function to Triton Kernel" />
<meta name="twitter:site" content="@supercharles" />
<meta name="twitter:creator" content="@Charles Zhu" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Charles Zhu"},"dateModified":"2025-08-07T17:02:08-07:00","datePublished":"2025-08-06T19:00:00-07:00","description":"An interactive deep dive into PyTorch’s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.","headline":"Torch.compile 101 - From Python Function to Triton Kernel","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/00/"},"url":"http://localhost:4000/posts/00/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Torch.compile 101 - From Python Function to Triton Kernel | ZCache
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="ZCache">
<meta name="application-name" content="ZCache">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script>







<script defer src="/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/favicons/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/">ZCache</a>
    <p class="site-subtitle fst-italic mb-0">SuperCharles's Engineering Blog</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/supercharles"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/supercharles"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['zhuchen1033','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>Torch.compile 101 - From Python Function to Triton Kernel</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>Torch.compile 101 - From Python Function to Triton Kernel</h1>
    
      <p class="post-desc fw-light mb-4">An interactive deep dive into PyTorch's torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.</p>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1754532000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Aug  6, 2025
</time>

      </span>

      <!-- lastmod date -->
      
        <span>
          Updated
          <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1754611328"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Aug  7, 2025
</time>

        </span>
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              
                
                
              
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="2136 words"
>
  <em>11 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">Torch.compile 101 - From Python Function to Triton Kernel</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">Torch.compile 101 - From Python Function to Triton Kernel</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <h2 id="introduction"><span class="me-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><code class="language-plaintext highlighter-rouge">torch.compile</code> in pytorch 2 is a common and revolutionary approach to optimizing PyTorch eager training performance. As AI Infra engineer, it is common case that we need to deal with benchmarking with different compiling configuration. Dirty tuning is usually fine and sometimes necessary, but more importantly, we want to deep dive and understand the underlying mechanism (which is also critical skills we want to build as system engineer!)</p>

<p>In this interactive deep dive, we’ll trace the journey of a simple Python function through the entire compilation pipeline, from the initial function definition to the final execution of optimized Triton kernels.</p>

<p><strong>🎯 Interactive Goal</strong>: By the end of this guide, you’ll be able to run commands and see the actual compilation process in action!</p>

<p><strong>📚 Recommended Reading</strong>: This will be a dirty code walkthrough guide. If you want to understand this from theory point of view, the <a href="https://docs.pytorch.org/assets/pytorch2-2.pdf">original pytorch 2 paper</a> is the best material in my opinion. Actually, I am using this paper personally as a starter point to understand the overall picture!</p>

<h2 id="phase-1-torchdynamo---graph-capture"><span class="me-2">Phase 1: TorchDynamo - Graph Capture</span><a href="#phase-1-torchdynamo---graph-capture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-torchdynamo"><span class="me-2">What is TorchDynamo?</span><a href="#what-is-torchdynamo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TorchDynamo is PyTorch’s graph capture mechanism that converts Python functions into computational graphs. It is hacking into the CPython’s compilation stage (<a href="https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html#pep-523-adding-a-frame-evaluation-api-to-cpython">reference</a>) and modify the bytecode to be actually executed.</p>

<p><a href="/assets/img/posts/dynamo.png" class="popup img-link shimmer"><img src="/assets/img/posts/dynamo.png" alt="TorchDynamo Architecture" loading="lazy"></a></p>

<h3 id="interactive-example-lets-trace-a-function"><span class="me-2">Interactive Example: Let’s Trace a Function</span><a href="#interactive-example-lets-trace-a-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">trace_example.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="c1"># torch version: 2.5.1+cu126
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="k">def</span> <span class="nf">simple_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A simple function to trace through TorchDynamo</span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Executing function...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># Create tensors
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== PHASE 1: TorchDynamo Graph Capture ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Calling torch.compile triggers TorchDynamo...</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># This is where the magic happens!
</span><span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">simple_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">First execution triggers compilation...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result shape: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="rouge-code"><pre><span class="nb">echo</span> <span class="s2">"Setting up environment variables for PyTorch Compile debugging..."</span>
<span class="nb">export </span><span class="nv">TORCH_COMPILE_DEBUG</span><span class="o">=</span>1
<span class="nb">export </span><span class="nv">TORCH_LOGS</span><span class="o">=</span>+dynamo,+inductor

<span class="nb">echo</span> <span class="s2">"Environment variables set:"</span>
<span class="nb">echo</span> <span class="s2">"TORCH_COMPILE_DEBUG=</span><span class="nv">$TORCH_COMPILE_DEBUG</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"TORCH_LOGS=</span><span class="nv">$TORCH_LOGS</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">""</span>
<span class="nb">echo</span> <span class="s2">"Running PyTorch Compile Deep Dive Demonstration..."</span>
<span class="nb">echo</span> <span class="s2">"================================================"</span>

python trace_example.py

<span class="nb">echo</span> <span class="s2">""</span>
<span class="nb">echo</span> <span class="s2">"Demo completed! Check the generated debug files in torch_compile_debug/"</span> 
</pre></td></tr></tbody></table></code></div></div>

<p><strong>What you’ll see:</strong></p>
<ul>
  <li>Detailed bytecode tracing output showing each instruction</li>
  <li>FX graph construction with node creation</li>
  <li>Guard generation for tensor shapes and types</li>
  <li>Variable tracking and symbolic execution</li>
</ul>

<h3 id="understanding-the-output"><span class="me-2">Understanding the Output</span><a href="#understanding-the-output" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>When you run the above, you’ll see output like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>V0801 22:21:47.817000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:864] [1/0] torchdynamo start compiling
V0801 22:21:47.818000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:865] [1/0] [__trace_source] TRACE starts_line
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE LOAD_FAST x []
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE LOAD_FAST y [LazyVariableTracker()]
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE BINARY_ADD None [LazyVariableTracker(), LazyVariableTracker()]
</pre></td></tr></tbody></table></code></div></div>

<p>This shows TorchDynamo analyzing each bytecode instruction and building the computational graph.</p>

<p>Let’s deep dive into each step separately!</p>

<h3 id="-deep-dive-torchdynamos-internal-architecture"><span class="me-2">🔬 Deep Dive: TorchDynamo’s Internal Architecture</span><a href="#-deep-dive-torchdynamos-internal-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Let’s dive deep into code base and check how it is tracing the program!</p>

<h4 id="entry-point"><span class="me-2"><strong>Entry Point</strong></span><a href="#entry-point" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>An execution of a Python program roughly consists of three stages:</p>

<ol>
  <li>Initialization: handles import, etc.</li>
  <li>Compilation: run lexing, parsing, etc to convert python code into bytecode for execution.</li>
  <li>Interpretation: run CPython VM to execute the python function</li>
</ol>

<p>The dynamo analysis was actually hacking into compilation stage, and rewrite the bytecode to be executed. When there is no already compiled function, we start a fresh analysis and tracing from scratch.</p>

<p>First step, it is using <code class="language-plaintext highlighter-rouge">dis</code> module to get the bytecode to be executed by CPython and run some initial cleanup of the Bytecode. You can check compiler <a href="https://github.com/python/cpython/blob/main/InternalDocs/compiler.md">here</a> on details of how it is generating the bytecode. Also <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/">here</a> is a great blog sharing how CPython is compiling and interpreting a python program</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># torch version: 2.8.0+cu126
</span>
<span class="c1"># From bytecode_transformation.py
</span><span class="k">def</span> <span class="nf">cleaned_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">safe</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Instruction</span><span class="p">]:</span>
    <span class="n">instructions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="n">convert_instruction</span><span class="p">,</span> <span class="n">dis</span><span class="p">.</span><span class="nf">get_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">)))</span>
    <span class="c1"># rest part are omitted
</span></pre></td></tr></tbody></table></code></div></div>

<h4 id="symbolic-execution-engine"><span class="me-2"><strong>Symbolic Execution Engine</strong></span><a href="#symbolic-execution-engine" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Instead of executing the bytecode via CPython interpreter, we created our own tracer to trace it. The core tracing happens in <code class="language-plaintext highlighter-rouge">symbolic_convert.py</code> through the <code class="language-plaintext highlighter-rouge">InstructionTranslator</code> class:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">class</span> <span class="nc">InstructionTranslatorBase</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Process exactly one instruction, return False we should exit</span><span class="sh">"""</span>
        <span class="n">ip</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">instruction_pointer</span>
        <span class="k">if</span> <span class="n">ip</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_instruction</span> <span class="o">=</span> <span class="n">inst</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">instructions</span><span class="p">[</span><span class="n">ip</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">instruction_pointer</span> <span class="o">=</span> <span class="n">ip</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="c1"># Dispatch to appropriate handler
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dispatch_table</span><span class="p">[</span><span class="n">inst</span><span class="p">.</span><span class="n">opcode</span><span class="p">](</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Instruction-by-Instruction Processing:</strong></p>
<ul>
  <li>Each Python bytecode instruction has a corresponding handler</li>
  <li>Variables are tracked as <code class="language-plaintext highlighter-rouge">VariableTracker</code> objects</li>
  <li>Stack management maintains symbolic state</li>
</ul>

<h4 id="bytecode-instruction-handlers"><span class="me-2"><strong>Bytecode Instruction Handlers</strong></span><a href="#bytecode-instruction-handlers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Each Python bytecode instruction has a specialized handler to generate output graph</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">def</span> <span class="nf">LOAD_FAST</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle loading local variables</span><span class="sh">"""</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_load_fast</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">STORE_FAST</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle storing local variables</span><span class="sh">"""</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_store_fast</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">CALL_FUNCTION</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle function calls</span><span class="sh">"""</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">popn</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">call_function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="p">{})</span>

<span class="k">def</span> <span class="nf">BINARY_ADD</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle addition operations</span><span class="sh">"""</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">popn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">call_method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sh">"</span><span class="s">__add__</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="p">{}))</span>
</pre></td></tr></tbody></table></code></div></div>

<h4 id="graph-construction-via-fx"><span class="me-2"><strong>Graph Construction via FX</strong></span><a href="#graph-construction-via-fx" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The <code class="language-plaintext highlighter-rouge">OutputGraph</code> class manages FX graph construction:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># From output_graph.py
</span><span class="k">class</span> <span class="nc">OutputGraph</span><span class="p">:</span>
    <span class="c1">#...
</span>    <span class="k">def</span> <span class="nf">create_proxy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Creates FX proxy objects for graph nodes
</span>    <span class="c1">#...
</span></pre></td></tr></tbody></table></code></div></div>

<h4 id="guard-generation"><span class="me-2"><strong>Guard Generation</strong></span><a href="#guard-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>TorchDynamo generates comprehensive guards to ensure correctness:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># From guards.py
</span><span class="k">def</span> <span class="nf">install_guard</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">guard_fn</span><span class="p">):</span>
    <span class="c1"># Install guards for tensor shapes, types, and values
</span>    <span class="c1"># Guards ensure graph validity across different inputs
</span></pre></td></tr></tbody></table></code></div></div>

<p><strong>Guard Types:</strong></p>
<ul>
  <li><strong>Shape Guards</strong>: Ensure tensor dimensions match</li>
  <li><strong>Type Guards</strong>: Ensure tensor dtypes are consistent</li>
  <li><strong>Value Guards</strong>: Ensure constant values match</li>
  <li><strong>Global Guards</strong>: Track changes to global state</li>
</ul>

<h4 id="graph-break-handling"><span class="me-2"><strong>Graph Break Handling</strong></span><a href="#graph-break-handling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>When TorchDynamo encounters unsupported operations, it creates graph breaks:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">def</span> <span class="nf">break_graph_if_unsupported</span><span class="p">(</span><span class="o">*</span><span class="p">,</span> <span class="n">push</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">decorator</span><span class="p">(</span><span class="n">inner_fn</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">Unsupported</span><span class="p">:</span>
                <span class="c1"># Create graph break and restart analysis
</span>                <span class="n">self</span><span class="p">.</span><span class="n">current_speculation</span><span class="p">.</span><span class="nf">fail_and_restart_analysis</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>

<h3 id="-torch-dynamo-summary"><span class="me-2">🎯 Torch Dynamo Summary</span><a href="#-torch-dynamo-summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>So far we have talked about overview of overall steps: byte-code execution, graph construction, graph-breaks, and guards. But the actual Torchdynamo is far more complicated than this overview (ex. important part is how AOTAutograd will work when we are tracing forward and backward pass, how do we handle mutations + functionalization, etc.) We will introduce some details on-demand in the following up series.</p>

<p>We now look into the phase 2 for hardware-specific optimization - TorchInductor</p>

<hr />

<h2 id="phase-2-torchinductor---graph-optimization"><span class="me-2">Phase 2: TorchInductor - Graph Optimization</span><a href="#phase-2-torchinductor---graph-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-torchinductor"><span class="me-2">What is TorchInductor?</span><a href="#what-is-torchinductor" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TorchInductor is PyTorch’s backend compiler that takes the FX graph from TorchDynamo and optimizes it for execution on various backends (CPU, CUDA, etc.). For CUDA, it generates Triton kernels.</p>

<h3 id="what-is-triton"><span class="me-2">What is Triton?</span><a href="#what-is-triton" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Triton is a language and compiler for writing highly optimized GPU kernels. It provides:</p>
<ul>
  <li><strong>High-level abstractions</strong> for GPU programming</li>
  <li><strong>Automatic optimization</strong> of memory access patterns</li>
  <li><strong>Parallel execution</strong> primitives</li>
  <li><strong>Integration</strong> with PyTorch</li>
</ul>

<h3 id="interactive-example-see-the-generation-process"><span class="me-2">Interactive Example: See the Generation Process</span><a href="#interactive-example-see-the-generation-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">inductor_example.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="k">def</span> <span class="nf">complex_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A function that will trigger various optimizations</span><span class="sh">"""</span>
    <span class="c1"># Multiple operations that can be fused
</span>    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="c1"># this will trigger decomposition
</span>    
    <span class="c1"># Reduction operations that trigger Triton code generation
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">e</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reduce along dimension 1
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>       <span class="c1"># Global mean
</span>    
    <span class="k">return</span> <span class="n">f</span>

<span class="c1"># Create larger tensors to see more optimization
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== PHASE 2: TorchInductor Optimization ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">This will show graph optimization and fusion...</span><span class="sh">"</span><span class="p">)</span>

<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">complex_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">First execution triggers Inductor optimization...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">TORCH_COMPILE_DEBUG</span><span class="o">=</span>1
<span class="nb">export </span><span class="nv">TORCH_LOGS</span><span class="o">=</span>+dynamo,+inductor

python inductor_example.py
</pre></td></tr></tbody></table></code></div></div>

<p><strong>What you’ll see:</strong></p>
<ul>
  <li>Graph lowering and buffer analysis</li>
  <li>Scheduler optimization</li>
  <li>Fusion analysis and kernel generation</li>
</ul>

<h3 id="check-generated-files"><span class="me-2">Check Generated Files</span><a href="#check-generated-files" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>After running the example, check the debug directory:</p>

<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c"># Find the latest debug directory</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"*.py"</span> <span class="nt">-o</span> <span class="nt">-name</span> <span class="s2">"*.txt"</span> | <span class="nb">head</span> <span class="nt">-10</span>
</pre></td></tr></tbody></table></code></div></div>

<p>This will show you the generated files including:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">output_code.py</code>: Generated Triton kernels</li>
  <li><code class="language-plaintext highlighter-rouge">ir_pre_fusion.txt</code>: IR before fusion</li>
  <li><code class="language-plaintext highlighter-rouge">ir_post_fusion.txt</code>: IR after fusion</li>
</ul>

<hr />

<h3 id="-deep-dive-torchinductors-triton-code-generation"><span class="me-2">🔬 Deep Dive: TorchInductor’s Triton Code Generation</span><a href="#-deep-dive-torchinductors-triton-code-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>First of all, when TorchDynamo has finished tracing the fx graph and reached the returning statement, it will trigger a <code class="language-plaintext highlighter-rouge">compile_subgraph</code> for further optimization</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_dynamo/symbolic_convert.py
# pytorch version: 2.8.0
</span><span class="k">class</span> <span class="nc">InstructionTranslator</span><span class="p">:</span>
    <span class="c1">#....
</span>    <span class="k">def</span> <span class="nf">_return</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inst</span><span class="p">):</span>
        <span class="c1">#...
</span>        <span class="n">all_stack_locals_metadata</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">.</span><span class="nf">compile_subgraph</span><span class="p">(</span>
            <span class="n">self</span><span class="p">,</span>
            <span class="n">reason</span><span class="o">=</span><span class="nc">GraphCompileReason</span><span class="p">(</span>
                <span class="sh">"</span><span class="s">return_value</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">frame_summary</span><span class="p">()],</span> <span class="n">graph_break</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># ...
</span></pre></td></tr></tbody></table></code></div></div>

<p>Then it will first convert the traced Fx graph into a decomposed Fx graph which only contains small set of supported operations via “<code class="language-plaintext highlighter-rouge">decomposition</code>”:</p>
<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_decomp/decomposition.py
# all of the decompositions are registered under this file
</span>
<span class="nd">@register_decomposition</span><span class="p">(</span><span class="n">aten</span><span class="p">.</span><span class="n">silu</span><span class="p">)</span>
<span class="nd">@out_wrapper</span><span class="p">()</span>
<span class="nd">@pw_cast_for_opmath</span>
<span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">self</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></div></div>

<p>After that, it will generate a lowered defined-by-run IR for TorchInductor. These IR will have the layout
information to execute the graph</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_inductor/compile_fx.py
</span><span class="k">class</span> <span class="nc">_InProcessFxCompile</span><span class="p">(</span><span class="n">FxCompile</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">codegen_and_compile</span><span class="p">():</span>
        <span class="c1">#...
</span>        <span class="n">graph</span> <span class="o">=</span> <span class="nc">GraphLowering</span><span class="p">(...)</span> <span class="c1"># this will generate the defined-by-run IR
</span></pre></td></tr></tbody></table></code></div></div>

<p>We can see the generated defined-by-run graph from <code class="language-plaintext highlighter-rouge">ir_pre_fusion.txt</code></p>
<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
</pre></td><td class="rouge-code"><pre>op0: SchedulerNode(ComputedBuffer)
op0.writes = [MemoryDep('buf0', c0, {c0: 1000})]
op0.unmet_dependencies = []
op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 1000000}), MemoryDep('arg1_1', c0, {c0: 1000000})]
op0.outputs = [
    buf0: ComputedBuffer
    buf0.layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
    buf0.users = [NodeUser(node=SchedulerNode(name='op1'), can_inplace=False, is_weak=False)]
]
op0.group.device = cuda:0
op0.group.iteration = (1000, 1000)
op0.sizes = ([1000], [1000])
arg0_1_layout = FixedLayout('cuda:0', torch.float32, size=[1000, 1000], stride=[1000, 1])
arg1_1_layout = FixedLayout('cuda:0', torch.float32, size=[1000, 1000], stride=[1000, 1])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
class op0_loop_body:
    var_ranges = {p0: 1000, p1: 1000}
    index0 = 1000*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg0_1', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('arg1_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(2.0, torch.float32)
        mul = ops.mul(add, constant)
        relu = ops.relu(mul)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', relu)
        get_index_2 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf0', get_index_2, reduction)
        return store_reduction


op1: SchedulerNode(ComputedBuffer)
op1.writes = [MemoryDep('buf1', 0, {})]
op1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 1000})]
op1.met_dependencies = []
op1.outputs = [
    buf1: ComputedBuffer
    buf1.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf1.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]
]
op1.group.device = cuda:0
op1.group.iteration = (1, 1000)
op1.sizes = ([], [1000])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
buf1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
class op1_loop_body:
    var_ranges = {p0: 1000}
    index0 = p0
    index1 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf0', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf1', get_index_1, reduction)
        return store_reduction


op2: SchedulerNode(ComputedBuffer)
op2.writes = [MemoryDep('buf2', 0, {})]
op2.unmet_dependencies = [MemoryDep('buf1', 0, {})]
op2.met_dependencies = []
op2.outputs = [
    buf2: ComputedBuffer
    buf2.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf2.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op2.group.device = cuda:0
op2.group.iteration = (1, 1)
op2.sizes = ([], [])
buf1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf2_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
class op2_loop_body:
    var_ranges = {}
    index0 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        constant = ops.constant(1000.0, torch.float32)
        truediv = ops.truediv(load, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf2', get_index_1, truediv, None)
        return store
</pre></td></tr></tbody></table></code></div></div>
<hr />

<h2 id="-appendix-debugging-and-inspection"><span class="me-2">🔍 Appendix: Debugging and Inspection</span><a href="#-appendix-debugging-and-inspection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="interactive-debug-commands"><span class="me-2">Interactive Debug Commands</span><a href="#interactive-debug-commands" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Here are some useful commands to explore the compilation process:</p>

<p><strong>1. View all generated files:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-type</span> f <span class="nt">-name</span> <span class="s2">"*.py"</span> <span class="nt">-o</span> <span class="nt">-name</span> <span class="s2">"*.txt"</span> | <span class="nb">sort</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>2. Examine the FX graph:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"fx_graph_*.py"</span> <span class="nt">-exec</span> <span class="nb">head</span> <span class="nt">-30</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>3. Check for Triton kernels:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"output_code.py"</span> <span class="nt">-exec</span> <span class="nb">wc</span> <span class="nt">-l</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>4. View IR transformations:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"ir_*.txt"</span> <span class="nt">-exec</span> <span class="nb">head</span> <span class="nt">-20</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<h2 id="-key-takeaways"><span class="me-2">🎓 Key Takeaways</span><a href="#-key-takeaways" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-youve-learned"><span class="me-2">What You’ve Learned</span><a href="#what-youve-learned" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ol>
  <li><strong>TorchDynamo</strong>: Converts Python functions to computational graphs through sophisticated bytecode analysis and symbolic execution</li>
  <li><strong>TorchInductor</strong>: Optimizes graphs and generates backend-specific code</li>
  <li><strong>Triton</strong>: Creates highly optimized GPU kernels</li>
</ol>

<h2 id="next-steps"><span class="me-2">Next Steps</span><a href="#next-steps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>In the next series, we will deep dive further into some common case handled by torch compile when we are running model training, including but not limited to:</p>
<ul>
  <li>AOT Autograd</li>
  <li>torch.compile and backward pass</li>
  <li>torch.comiple and Distributed Data Parallel (DDP)</li>
  <li>torch.compile and Fully Sharded Distributed Parallel (FSDP2)</li>
</ul>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/machine-learning/">Machine Learning</a>,
          <a href="/categories/pytorch/">PyTorch</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/pytorch/"
            class="post-tag no-text-decoration"
          >pytorch</a>
        
          <a
            href="/tags/torch-compile/"
            class="post-tag no-text-decoration"
          >torch.compile</a>
        
          <a
            href="/tags/triton/"
            class="post-tag no-text-decoration"
          >triton</a>
        
          <a
            href="/tags/optimization/"
            class="post-tag no-text-decoration"
          >optimization</a>
        
          <a
            href="/tags/deep-learning/"
            class="post-tag no-text-decoration"
          >deep-learning</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Torch.compile%20101%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F00%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Torch.compile%20101%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F00%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F00%2F&text=Torch.compile%20101%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/00/">Torch.compile 101 - From Python Function to Triton Kernel</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/optimization/">optimization</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/torch-compile/">torch.compile</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/triton/">triton</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Older">
      <p>-</p>
    </div>
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Newer">
      <p>-</p>
    </div>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2025</time>

    
      <a href="https://twitter.com/supercharles">supercharles</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.3.1"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/optimization/">optimization</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/torch-compile/">torch.compile</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/triton/">triton</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

