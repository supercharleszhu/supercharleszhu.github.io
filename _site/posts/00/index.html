<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Torch.compile Deep Dive - From Python Function to Triton Kernel" />
<meta name="author" content="Charles Zhu" />
<meta property="og:locale" content="en" />
<meta name="description" content="An interactive deep dive into PyTorch’s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis." />
<meta property="og:description" content="An interactive deep dive into PyTorch’s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis." />
<link rel="canonical" href="http://localhost:4000/posts/00/" />
<meta property="og:url" content="http://localhost:4000/posts/00/" />
<meta property="og:site_name" content="ZCache" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-06T19:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Torch.compile Deep Dive - From Python Function to Triton Kernel" />
<meta name="twitter:site" content="@supercharles" />
<meta name="twitter:creator" content="@Charles Zhu" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Charles Zhu"},"dateModified":"2025-08-06T19:00:00-07:00","datePublished":"2025-08-06T19:00:00-07:00","description":"An interactive deep dive into PyTorch’s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.","headline":"Torch.compile Deep Dive - From Python Function to Triton Kernel","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/posts/00/"},"url":"http://localhost:4000/posts/00/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Torch.compile Deep Dive - From Python Function to Triton Kernel | ZCache
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="ZCache">
<meta name="application-name" content="ZCache">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script>







<script defer src="/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/favicons/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/">ZCache</a>
    <p class="site-subtitle fst-italic mb-0">SuperCharles's Engineering Blog</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/supercharles"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/supercharles"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['zhuchen1033','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>Torch.compile Deep Dive - From Python Function to Triton Kernel</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>Torch.compile Deep Dive - From Python Function to Triton Kernel</h1>
    
      <p class="post-desc fw-light mb-4">An interactive deep dive into PyTorch's torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.</p>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1754532000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Aug  6, 2025
</time>

      </span>

      <!-- lastmod date -->
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              
                
                
              
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="3354 words"
>
  <em>18 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">Torch.compile Deep Dive - From Python Function to Triton Kernel</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">Torch.compile Deep Dive - From Python Function to Triton Kernel</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <h1 id="torchcompile-deep-dive-from-python-function-to-triton-kernel">Torch.compile Deep Dive: From Python Function to Triton Kernel</h1>

<h2 id="introduction"><span class="me-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>PyTorch’s <code class="language-plaintext highlighter-rouge">torch.compile</code> represents a revolutionary approach to optimizing PyTorch code. In this interactive deep dive, we’ll trace the journey of a simple Python function through the entire compilation pipeline, from the initial function definition to the final execution of optimized Triton kernels.</p>

<p><strong>🎯 Interactive Goal</strong>: By the end of this guide, you’ll be able to run commands and see the actual compilation process in action!</p>

<hr />

<h2 id="-quick-start-run-the-complete-demo"><span class="me-2">🚀 Quick Start: Run the Complete Demo</span><a href="#-quick-start-run-the-complete-demo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>First, let’s run a complete demonstration to see the entire pipeline in action:</p>

<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="c"># Clone or download the demo files, then run:</span>
<span class="nb">chmod</span> +x run_torch_compile_demo.sh
./run_torch_compile_demo.sh
</pre></td></tr></tbody></table></code></div></div>

<p>This will show you the complete pipeline with performance comparisons. Now let’s break down each phase step by step.</p>

<hr />

<h2 id="phase-1-torchdynamo---graph-capture"><span class="me-2">Phase 1: TorchDynamo - Graph Capture</span><a href="#phase-1-torchdynamo---graph-capture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-torchdynamo"><span class="me-2">What is TorchDynamo?</span><a href="#what-is-torchdynamo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TorchDynamo is PyTorch’s graph capture mechanism that converts Python functions into computational graphs. It is hacking into the CPython’s compilation stage (<a href="https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html#pep-523-adding-a-frame-evaluation-api-to-cpython">reference</a> and modify the bytecode to be actually executed.</p>

<p><a href="/assets/img/posts/dynamo.png" class="popup img-link shimmer"><img src="/assets/img/posts/dynamo.png" alt="TorchDynamo Architecture" loading="lazy"></a></p>

<h3 id="-deep-dive-torchdynamos-internal-architecture"><span class="me-2">🔬 Deep Dive: TorchDynamo’s Internal Architecture</span><a href="#-deep-dive-torchdynamos-internal-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Let’s dive deep into code base and check how it is tracing the program!</p>

<h4 id="entry-point"><span class="me-2"><strong>Entry Point</strong></span><a href="#entry-point" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>An execution of a Python program roughly consists of three stages:</p>

<ol>
  <li>Initialization</li>
  <li>Compilation</li>
  <li>lInterpretation</li>
</ol>

<p>The dynamo analysis was actually hacking into compilation stage, and rewrite the bytecode to be executed. When there is no already compiled function, we start a fresh analysis and tracing from scratch.</p>

<p>First step, it is using <code class="language-plaintext highlighter-rouge">dis</code> module to get the bytecode to be executed by CPython and run some initial cleanup of the Bytecode. You can check compiler <a href="https://github.com/python/cpython/blob/main/InternalDocs/compiler.md">here</a> on details of how it is generating the bytecode. Also <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/">here</a> is a great blog sharing how CPython is compiling and interpreting a python program</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># torch version: 2.5.1+cu126
</span>
<span class="c1"># From bytecode_transformation.py
</span><span class="k">def</span> <span class="nf">cleaned_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">safe</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Instruction</span><span class="p">]:</span>
    <span class="n">instructions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="n">convert_instruction</span><span class="p">,</span> <span class="n">dis</span><span class="p">.</span><span class="nf">get_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">)))</span>
    <span class="c1"># rest part are omitted
</span></pre></td></tr></tbody></table></code></div></div>

<h4 id="symbolic-execution-engine"><span class="me-2"><strong>Symbolic Execution Engine</strong></span><a href="#symbolic-execution-engine" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The core tracing happens in <code class="language-plaintext highlighter-rouge">symbolic_convert.py</code> through the <code class="language-plaintext highlighter-rouge">InstructionTranslator</code> class:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">class</span> <span class="nc">InstructionTranslatorBase</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Process exactly one instruction, return False we should exit</span><span class="sh">"""</span>
        <span class="n">ip</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">instruction_pointer</span>
        <span class="k">if</span> <span class="n">ip</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_instruction</span> <span class="o">=</span> <span class="n">inst</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">instructions</span><span class="p">[</span><span class="n">ip</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">instruction_pointer</span> <span class="o">=</span> <span class="n">ip</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="c1"># Dispatch to appropriate handler
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dispatch_table</span><span class="p">[</span><span class="n">inst</span><span class="p">.</span><span class="n">opcode</span><span class="p">](</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Instruction-by-Instruction Processing:</strong></p>
<ul>
  <li>Each Python bytecode instruction has a corresponding handler</li>
  <li>Variables are tracked as <code class="language-plaintext highlighter-rouge">VariableTracker</code> objects</li>
  <li>Stack management maintains symbolic state</li>
</ul>

<h4 id="4-variable-tracking-system"><span class="me-2"><strong>4. Variable Tracking System</strong></span><a href="#4-variable-tracking-system" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>TorchDynamo uses a sophisticated variable tracking hierarchy:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="c1"># From variables/base.py
</span><span class="k">class</span> <span class="nc">VariableTracker</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">call_function</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tx</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">VariableTracker</span><span class="sh">"</span><span class="p">:</span>
        <span class="c1"># Default implementation - can be overridden by subclasses
</span>        
    <span class="k">def</span> <span class="nf">call_method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tx</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">VariableTracker</span><span class="sh">"</span><span class="p">:</span>
        <span class="c1"># Handle method calls on tracked objects
</span></pre></td></tr></tbody></table></code></div></div>

<p><strong>Key Variable Types:</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">TensorVariable</code>: Represents PyTorch tensors</li>
  <li><code class="language-plaintext highlighter-rouge">ConstantVariable</code>: Represents Python constants</li>
  <li><code class="language-plaintext highlighter-rouge">ListVariable</code>, <code class="language-plaintext highlighter-rouge">TupleVariable</code>: Represents sequences</li>
  <li><code class="language-plaintext highlighter-rouge">NNModuleVariable</code>: Represents neural network modules</li>
  <li><code class="language-plaintext highlighter-rouge">BuiltinVariable</code>: Represents Python builtins</li>
</ul>

<h4 id="5-bytecode-instruction-handlers"><span class="me-2"><strong>5. Bytecode Instruction Handlers</strong></span><a href="#5-bytecode-instruction-handlers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Each Python bytecode instruction has a specialized handler:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">def</span> <span class="nf">LOAD_FAST</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle loading local variables</span><span class="sh">"""</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_load_fast</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">STORE_FAST</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle storing local variables</span><span class="sh">"""</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_store_fast</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">CALL_FUNCTION</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle function calls</span><span class="sh">"""</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">popn</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">call_function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="p">{})</span>

<span class="k">def</span> <span class="nf">BINARY_ADD</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle addition operations</span><span class="sh">"""</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">popn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">call_method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sh">"</span><span class="s">__add__</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="p">{}))</span>
</pre></td></tr></tbody></table></code></div></div>

<h4 id="6-graph-construction-via-fx"><span class="me-2"><strong>6. Graph Construction via FX</strong></span><a href="#6-graph-construction-via-fx" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The <code class="language-plaintext highlighter-rouge">OutputGraph</code> class manages FX graph construction:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre><span class="c1"># From output_graph.py
</span><span class="k">class</span> <span class="nc">OutputGraph</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">create_proxy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Creates FX proxy objects for graph nodes
</span>        
    <span class="k">def</span> <span class="nf">compile_subgraph</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">tx</span><span class="p">,</span> <span class="n">partial_convert</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">reason</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># Compiles the current subgraph into an FX GraphModule
</span></pre></td></tr></tbody></table></code></div></div>

<p><strong>Key Components:</strong></p>
<ul>
  <li><strong>FX Graph</strong>: Built incrementally as instructions are processed</li>
  <li><strong>Proxy Objects</strong>: Represent symbolic computations</li>
  <li><strong>Guards</strong>: Generated to ensure graph validity across different inputs</li>
</ul>

<h4 id="7-graph-break-handling"><span class="me-2"><strong>7. Graph Break Handling</strong></span><a href="#7-graph-break-handling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>When TorchDynamo encounters unsupported operations, it creates graph breaks:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">def</span> <span class="nf">break_graph_if_unsupported</span><span class="p">(</span><span class="o">*</span><span class="p">,</span> <span class="n">push</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">decorator</span><span class="p">(</span><span class="n">inner_fn</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">Unsupported</span><span class="p">:</span>
                <span class="c1"># Create graph break and restart analysis
</span>                <span class="n">self</span><span class="p">.</span><span class="n">current_speculation</span><span class="p">.</span><span class="nf">fail_and_restart_analysis</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>

<h4 id="8-speculation-and-restart-system"><span class="me-2"><strong>8. Speculation and Restart System</strong></span><a href="#8-speculation-and-restart-system" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>TorchDynamo uses a sophisticated speculation system:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">class</span> <span class="nc">SpeculationLog</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">restart</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># Restart the dynamo conversion process from the beginning
</span>        <span class="c1"># When hitting the start of speculation that failed, generate a graph break
</span></pre></td></tr></tbody></table></code></div></div>

<p><strong>Speculation Process:</strong></p>
<ol>
  <li><strong>Checkpoint Creation</strong>: Save current state</li>
  <li><strong>Optimistic Execution</strong>: Try to continue with current assumptions</li>
  <li><strong>Failure Detection</strong>: If assumptions fail, restart from checkpoint</li>
  <li><strong>Graph Break Generation</strong>: Insert break points where needed</li>
</ol>

<h4 id="9-guard-generation"><span class="me-2"><strong>9. Guard Generation</strong></span><a href="#9-guard-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>TorchDynamo generates comprehensive guards to ensure correctness:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># From guards.py
</span><span class="k">def</span> <span class="nf">install_guard</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">guard_fn</span><span class="p">):</span>
    <span class="c1"># Install guards for tensor shapes, types, and values
</span>    <span class="c1"># Guards ensure graph validity across different inputs
</span></pre></td></tr></tbody></table></code></div></div>

<p><strong>Guard Types:</strong></p>
<ul>
  <li><strong>Shape Guards</strong>: Ensure tensor dimensions match</li>
  <li><strong>Type Guards</strong>: Ensure tensor dtypes are consistent</li>
  <li><strong>Value Guards</strong>: Ensure constant values match</li>
  <li><strong>Global Guards</strong>: Track changes to global state</li>
</ul>

<h4 id="post-processing"><span class="me-2"><strong>Post Processing</strong></span><a href="#post-processing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Instead of directly letting CPython executing the bytecode, TorchDynamo runs a tracer via</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="c1"># From convert_frame.py
</span><span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">instructions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Instruction</span><span class="p">],</span> <span class="n">code_options</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">object</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Remove dead code and pointless jumps
</span>    <span class="n">instructions</span><span class="p">[:]</span> <span class="o">=</span> <span class="nf">remove_dead_code</span><span class="p">(</span><span class="n">instructions</span><span class="p">)</span>
    <span class="n">instructions</span><span class="p">[:]</span> <span class="o">=</span> <span class="nf">remove_pointless_jumps</span><span class="p">(</span><span class="n">instructions</span><span class="p">)</span>
    
    <span class="c1"># Process exception tables for Python 3.11+
</span>    <span class="nf">propagate_inst_exn_table_entries</span><span class="p">(</span><span class="n">instructions</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Key Transformations:</strong></p>
<ul>
  <li><strong>Dead Code Elimination</strong>: Removes unreachable instructions</li>
  <li><strong>Jump Optimization</strong>: Simplifies control flow</li>
  <li><strong>Exception Table Processing</strong>: Handles try/except blocks for Python 3.11+</li>
</ul>

<h3 id="interactive-example-lets-trace-a-function"><span class="me-2">Interactive Example: Let’s Trace a Function</span><a href="#interactive-example-lets-trace-a-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">trace_example.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="c1"># Set environment variables for detailed debugging
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_COMPILE_DEBUG</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">+dynamo,+inductor</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">simple_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A simple function to trace through TorchDynamo</span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Executing function...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">c</span>

<span class="c1"># Create tensors
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== PHASE 1: TorchDynamo Graph Capture ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Calling torch.compile triggers TorchDynamo...</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># This is where the magic happens!
</span><span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">simple_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">First execution triggers compilation...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result shape: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python trace_example.py
</pre></td></tr></tbody></table></code></div></div>

<p><strong>What you’ll see:</strong></p>
<ul>
  <li>Detailed bytecode tracing output showing each instruction</li>
  <li>FX graph construction with node creation</li>
  <li>Guard generation for tensor shapes and types</li>
  <li>Variable tracking and symbolic execution</li>
</ul>

<h3 id="understanding-the-output"><span class="me-2">Understanding the Output</span><a href="#understanding-the-output" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>When you run the above, you’ll see output like this:</p>

<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre>V0801 22:21:47.817000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:864] [1/0] torchdynamo start compiling
V0801 22:21:47.818000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:865] [1/0] [__trace_source] TRACE starts_line
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE LOAD_FAST x []
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE LOAD_FAST y [LazyVariableTracker()]
V0801 22:21:47.819000 393353 .local/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:888] [1/0] [__trace_bytecode] TRACE BINARY_ADD None [LazyVariableTracker(), LazyVariableTracker()]
</pre></td></tr></tbody></table></code></div></div>

<p>This shows TorchDynamo analyzing each bytecode instruction and building the computational graph.</p>

<h3 id="-advanced-examining-the-internal-process"><span class="me-2">🔍 Advanced: Examining the Internal Process</span><a href="#-advanced-examining-the-internal-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>To see the detailed internal process, create <code class="language-plaintext highlighter-rouge">advanced_trace.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">dis</span>

<span class="c1"># Set environment variables for maximum debugging
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_COMPILE_DEBUG</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">+dynamo,+inductor</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS_LEVEL</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">DEBUG</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">advanced_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Function to demonstrate advanced TorchDynamo features</span><span class="sh">"""</span>
    <span class="c1"># Show bytecode analysis
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== Bytecode Analysis ===</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">dis</span><span class="p">.</span><span class="nf">dis</span><span class="p">(</span><span class="n">advanced_function</span><span class="p">)</span>
    
    <span class="c1"># Multiple operations to trace
</span>    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">e</span>

<span class="c1"># Create tensors
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== ADVANCED TORCHDYNAMO ANALYSIS ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">This will show detailed internal process...</span><span class="sh">"</span><span class="p">)</span>

<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">advanced_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">First execution with detailed tracing...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">=== INTERNAL PROCESS SUMMARY ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">1. Bytecode Analysis: Function bytecode was analyzed</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">2. Symbolic Execution: Variables tracked as VariableTracker objects</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">3. Graph Construction: FX graph built incrementally</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">4. Guard Generation: Guards created for tensor shapes/types</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">5. Graph Break Handling: Unsupported operations handled gracefully</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run the advanced example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python advanced_trace.py
</pre></td></tr></tbody></table></code></div></div>

<h3 id="-key-torchdynamo-concepts"><span class="me-2">🎯 Key TorchDynamo Concepts</span><a href="#-key-torchdynamo-concepts" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p><strong>1. Symbolic Execution:</strong></p>
<ul>
  <li>Variables are tracked as <code class="language-plaintext highlighter-rouge">VariableTracker</code> objects, not concrete values</li>
  <li>Operations are recorded symbolically in the FX graph</li>
  <li>This allows for optimization and fusion opportunities</li>
</ul>

<p><strong>2. Graph Breaks:</strong></p>
<ul>
  <li>When TorchDynamo encounters unsupported operations, it creates graph breaks</li>
  <li>The function is split into multiple graphs at break points</li>
  <li>This ensures compatibility while still providing optimization benefits</li>
</ul>

<p><strong>3. Guard Generation:</strong></p>
<ul>
  <li>Guards ensure the compiled graph is valid for different inputs</li>
  <li>Guards check tensor shapes, types, and other properties</li>
  <li>If guards fail, the graph is recompiled with new assumptions</li>
</ul>

<p><strong>4. Speculation:</strong></p>
<ul>
  <li>TorchDynamo makes optimistic assumptions about code behavior</li>
  <li>If assumptions fail, it restarts analysis with more conservative approach</li>
  <li>This balances performance with correctness</li>
</ul>

<p>This sophisticated architecture allows TorchDynamo to efficiently trace Python code, handle complex Python constructs, and generate optimized computational graphs while maintaining correctness through comprehensive guard generation.</p>

<hr />

<h2 id="phase-2-torchinductor---graph-optimization"><span class="me-2">Phase 2: TorchInductor - Graph Optimization</span><a href="#phase-2-torchinductor---graph-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-torchinductor"><span class="me-2">What is TorchInductor?</span><a href="#what-is-torchinductor" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TorchInductor is PyTorch’s backend compiler that takes the FX graph from TorchDynamo and optimizes it for execution on various backends (CPU, CUDA, etc.). For CUDA, it generates Triton kernels.</p>

<h3 id="interactive-example-see-the-optimization-process"><span class="me-2">Interactive Example: See the Optimization Process</span><a href="#interactive-example-see-the-optimization-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">inductor_example.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="c1"># Set environment variables for detailed debugging
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_COMPILE_DEBUG</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">+dynamo,+inductor</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">complex_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A function that will trigger various optimizations</span><span class="sh">"""</span>
    <span class="c1"># Multiple operations that can be fused
</span>    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    
    <span class="c1"># Reduction operations that trigger Triton code generation
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reduce along dimension 1
</span>    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>       <span class="c1"># Global mean
</span>    
    <span class="k">return</span> <span class="n">e</span>

<span class="c1"># Create larger tensors to see more optimization
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== PHASE 2: TorchInductor Optimization ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">This will show graph optimization and fusion...</span><span class="sh">"</span><span class="p">)</span>

<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">complex_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">First execution triggers Inductor optimization...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python inductor_example.py
</pre></td></tr></tbody></table></code></div></div>

<p><strong>What you’ll see:</strong></p>
<ul>
  <li>Graph lowering and buffer analysis</li>
  <li>Scheduler optimization</li>
  <li>Fusion analysis and kernel generation</li>
</ul>

<h3 id="check-generated-files"><span class="me-2">Check Generated Files</span><a href="#check-generated-files" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>After running the example, check the debug directory:</p>

<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c"># Find the latest debug directory</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"*.py"</span> <span class="nt">-o</span> <span class="nt">-name</span> <span class="s2">"*.txt"</span> | <span class="nb">head</span> <span class="nt">-10</span>
</pre></td></tr></tbody></table></code></div></div>

<p>This will show you the generated files including:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">output_code.py</code>: Generated Triton kernels</li>
  <li><code class="language-plaintext highlighter-rouge">ir_pre_fusion.txt</code>: IR before fusion</li>
  <li><code class="language-plaintext highlighter-rouge">ir_post_fusion.txt</code>: IR after fusion</li>
</ul>

<hr />

<h2 id="phase-3-triton-kernel-generation"><span class="me-2">Phase 3: Triton Kernel Generation</span><a href="#phase-3-triton-kernel-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-triton"><span class="me-2">What is Triton?</span><a href="#what-is-triton" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Triton is a language and compiler for writing highly optimized GPU kernels. It provides:</p>
<ul>
  <li><strong>High-level abstractions</strong> for GPU programming</li>
  <li><strong>Automatic optimization</strong> of memory access patterns</li>
  <li><strong>Parallel execution</strong> primitives</li>
  <li><strong>Integration</strong> with PyTorch</li>
</ul>

<h3 id="interactive-example-force-triton-code-generation"><span class="me-2">Interactive Example: Force Triton Code Generation</span><a href="#interactive-example-force-triton-code-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">triton_example.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="c1"># Set environment variables for detailed debugging
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_COMPILE_DEBUG</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">+dynamo,+inductor</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">triton_trigger_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    This function is designed to trigger Triton code generation.
    Uses operations that benefit from custom kernels.
    </span><span class="sh">"""</span>
    <span class="c1"># Multiple operations that can be fused into a single kernel
</span>    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    
    <span class="c1"># Reduction operations that definitely trigger Triton
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reduce along dimension 1
</span>    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>       <span class="c1"># Global mean
</span>    
    <span class="c1"># More complex operations
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="mf">10.0</span>
    
    <span class="k">return</span> <span class="n">g</span>

<span class="c1"># Create tensors with specific shapes to trigger Triton compilation
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== PHASE 3: Triton Kernel Generation ===</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">This should generate actual Triton kernels...</span><span class="sh">"</span><span class="p">)</span>

<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">triton_trigger_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">First execution triggers Triton kernel generation...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Triton code should have been generated!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Check the debug directory for generated files.</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python triton_example.py
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Check for Triton kernels:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c"># Look for output_code.py files</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"output_code.py"</span> <span class="nt">-exec</span> <span class="nb">ls</span> <span class="nt">-la</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Examine the generated Triton code:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c"># View the generated Triton kernel</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"output_code.py"</span> <span class="nt">-exec</span> <span class="nb">head</span> <span class="nt">-20</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<hr />

<h2 id="phase-4-runtime-execution"><span class="me-2">Phase 4: Runtime Execution</span><a href="#phase-4-runtime-execution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="interactive-performance-comparison"><span class="me-2">Interactive Performance Comparison</span><a href="#interactive-performance-comparison" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">performance_comparison.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="c1"># Set environment variables for detailed debugging
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_COMPILE_DEBUG</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">+dynamo,+inductor</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">benchmark_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Function to benchmark</span><span class="sh">"""</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">e</span>

<span class="c1"># Create tensors
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=== PHASE 4: Runtime Performance Comparison ===</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Benchmark original function
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Benchmarking original function...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">result_original</span> <span class="o">=</span> <span class="nf">benchmark_function</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">original_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original function time: </span><span class="si">{</span><span class="n">original_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Benchmark compiled function
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Benchmarking compiled function...</span><span class="sh">"</span><span class="p">)</span>
<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">benchmark_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># First run (includes compilation time)
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">result_compiled</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">first_run_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">First run (with compilation): </span><span class="si">{</span><span class="n">first_run_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Subsequent runs (optimized execution)
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">result_compiled</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">compiled_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Subsequent runs: </span><span class="si">{</span><span class="n">compiled_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Performance analysis
</span><span class="n">speedup</span> <span class="o">=</span> <span class="n">original_time</span> <span class="o">/</span> <span class="p">(</span><span class="n">compiled_time</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">improvement</span> <span class="o">=</span> <span class="p">((</span><span class="n">original_time</span> <span class="o">-</span> <span class="p">(</span><span class="n">compiled_time</span> <span class="o">/</span> <span class="mi">10</span><span class="p">))</span> <span class="o">/</span> <span class="n">original_time</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="si">{</span><span class="sh">'</span><span class="s">=</span><span class="sh">'</span><span class="o">*</span><span class="mi">50</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">PERFORMANCE ANALYSIS</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="sh">'</span><span class="s">=</span><span class="sh">'</span><span class="o">*</span><span class="mi">50</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original execution time: </span><span class="si">{</span><span class="n">original_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Compiled execution time: </span><span class="si">{</span><span class="n">compiled_time</span><span class="o">/</span><span class="mi">10</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Performance improvement: </span><span class="si">{</span><span class="n">improvement</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">speedup</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Compilation provided performance benefits!</span><span class="sh">"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">⚠️  Compilation did not provide performance benefits in this case.</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python performance_comparison.py
</pre></td></tr></tbody></table></code></div></div>

<hr />

<h2 id="-debugging-and-inspection"><span class="me-2">🔍 Debugging and Inspection</span><a href="#-debugging-and-inspection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="interactive-debug-commands"><span class="me-2">Interactive Debug Commands</span><a href="#interactive-debug-commands" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Here are some useful commands to explore the compilation process:</p>

<p><strong>1. View all generated files:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-type</span> f <span class="nt">-name</span> <span class="s2">"*.py"</span> <span class="nt">-o</span> <span class="nt">-name</span> <span class="s2">"*.txt"</span> | <span class="nb">sort</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>2. Examine the FX graph:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"fx_graph_*.py"</span> <span class="nt">-exec</span> <span class="nb">head</span> <span class="nt">-30</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>3. Check for Triton kernels:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"output_code.py"</span> <span class="nt">-exec</span> <span class="nb">wc</span> <span class="nt">-l</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>4. View IR transformations:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"ir_*.txt"</span> <span class="nt">-exec</span> <span class="nb">head</span> <span class="nt">-20</span> <span class="o">{}</span> <span class="se">\;</span>
</pre></td></tr></tbody></table></code></div></div>

<h3 id="interactive-debug-script"><span class="me-2">Interactive Debug Script</span><a href="#interactive-debug-script" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">debug_explorer.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
</pre></td><td class="rouge-code"><pre><span class="c1">#!/usr/bin/env python3
</span><span class="sh">"""</span><span class="s">
Interactive debug explorer for PyTorch compile
</span><span class="sh">"""</span>

<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">glob</span>
<span class="kn">import</span> <span class="n">subprocess</span>

<span class="k">def</span> <span class="nf">find_latest_debug_dir</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Find the latest debug directory</span><span class="sh">"""</span>
    <span class="n">debug_dirs</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="sh">"</span><span class="s">torch_compile_debug/run_*/torchinductor/*/</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">debug_dirs</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="n">debug_dirs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getctime</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">explore_debug_files</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Explore the generated debug files</span><span class="sh">"""</span>
    <span class="n">debug_dir</span> <span class="o">=</span> <span class="nf">find_latest_debug_dir</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">debug_dir</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">❌ No debug directories found</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">🔍 Exploring debug directory: </span><span class="si">{</span><span class="n">debug_dir</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># List all files
</span>    <span class="n">files</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">debug_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">*</span><span class="sh">"</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">📁 Generated files:</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">files</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isfile</span><span class="p">(</span><span class="nb">file</span><span class="p">):</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">getsize</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span><span class="si">}</span><span class="s"> (</span><span class="si">{</span><span class="n">size</span><span class="si">}</span><span class="s"> bytes)</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># Check for Triton kernels
</span>    <span class="n">output_code_files</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">debug_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">output_code.py</span><span class="sh">"</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">output_code_files</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">✅ Triton kernels generated!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">output_code_files</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  📄 </span><span class="si">{</span><span class="nb">file</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            
        <span class="c1"># Show snippet of Triton code
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">📝 Triton kernel snippet:</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">with</span> <span class="nf">open</span><span class="p">(</span><span class="n">output_code_files</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="sh">'</span><span class="s">r</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">lines</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="nf">readlines</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">line</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">lines</span><span class="p">[:</span><span class="mi">10</span><span class="p">]):</span>
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="mi">2</span><span class="n">d</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">line</span><span class="p">.</span><span class="nf">rstrip</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
            <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">10</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  ...</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">❌ No Triton kernels generated</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Check for IR files
</span>    <span class="n">ir_files</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">debug_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">ir_*.txt</span><span class="sh">"</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">ir_files</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">📊 IR files found: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">ir_files</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="nb">file</span> <span class="ow">in</span> <span class="n">ir_files</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">  📄 </span><span class="si">{</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">basename</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">🔍 PyTorch Compile Debug Explorer</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nf">explore_debug_files</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run the debug explorer:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python debug_explorer.py
</pre></td></tr></tbody></table></code></div></div>

<hr />

<h2 id="-complete-interactive-demo"><span class="me-2">🎯 Complete Interactive Demo</span><a href="#-complete-interactive-demo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="one-command-demo"><span class="me-2">One-Command Demo</span><a href="#one-command-demo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">complete_demo.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
</pre></td><td class="rouge-code"><pre><span class="c1">#!/usr/bin/env python3
</span><span class="sh">"""</span><span class="s">
Complete interactive demo of PyTorch compile pipeline
</span><span class="sh">"""</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">glob</span>

<span class="c1"># Set environment variables
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_COMPILE_DEBUG</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">1</span><span class="sh">'</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">TORCH_LOGS</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">+dynamo,+inductor</span><span class="sh">'</span>

<span class="k">def</span> <span class="nf">demo_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Function for the complete demo</span><span class="sh">"""</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">e</span>

<span class="k">def</span> <span class="nf">run_complete_demo</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">Run the complete interactive demo</span><span class="sh">"""</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">🚀 PyTorch Compile Complete Interactive Demo</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    
    <span class="c1"># Check CUDA availability
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">❌ CUDA not available. This demo requires CUDA.</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">✅ CUDA available: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">get_device_name</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">📦 PyTorch version: </span><span class="si">{</span><span class="n">torch</span><span class="p">.</span><span class="n">__version__</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Create tensors
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">📦 Creating input tensors...</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Input shapes: </span><span class="si">{</span><span class="n">input1</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">, </span><span class="si">{</span><span class="n">input2</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Phase 1: TorchDynamo
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">🔄 PHASE 1: TorchDynamo Graph Capture</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   Calling torch.compile triggers TorchDynamo...</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">demo_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Phase 2-3: TorchInductor + Triton
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">⚡ PHASE 2-3: TorchInductor Optimization + Triton Generation</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   First execution triggers compilation...</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">compilation_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Compilation + first execution time: </span><span class="si">{</span><span class="n">compilation_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Phase 4: Performance comparison
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">🏃 PHASE 4: Performance Comparison</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Benchmark original
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="nf">demo_function</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">original_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    
    <span class="c1"># Benchmark compiled
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">_</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">compiled_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    
    <span class="c1"># Analysis
</span>    <span class="n">speedup</span> <span class="o">=</span> <span class="n">original_time</span> <span class="o">/</span> <span class="n">compiled_time</span>
    <span class="n">improvement</span> <span class="o">=</span> <span class="p">((</span><span class="n">original_time</span> <span class="o">-</span> <span class="n">compiled_time</span><span class="p">)</span> <span class="o">/</span> <span class="n">original_time</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Original execution time: </span><span class="si">{</span><span class="n">original_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Compiled execution time: </span><span class="si">{</span><span class="n">compiled_time</span><span class="si">:</span><span class="p">.</span><span class="mi">6</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Speedup: </span><span class="si">{</span><span class="n">speedup</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Performance improvement: </span><span class="si">{</span><span class="n">improvement</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">speedup</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   ✅ Compilation provided performance benefits!</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   ⚠️  Compilation did not provide performance benefits.</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Debug information
</span>    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">🔍 DEBUG INFORMATION</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">debug_dirs</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="sh">"</span><span class="s">torch_compile_debug/run_*/torchinductor/*/</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">debug_dirs</span><span class="p">:</span>
        <span class="n">latest_dir</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">debug_dirs</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">getctime</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">   Latest debug directory: </span><span class="si">{</span><span class="n">latest_dir</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        
        <span class="c1"># Check for Triton kernels
</span>        <span class="n">output_code_files</span> <span class="o">=</span> <span class="n">glob</span><span class="p">.</span><span class="nf">glob</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">latest_dir</span><span class="p">,</span> <span class="sh">"</span><span class="s">output_code.py</span><span class="sh">"</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">output_code_files</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   ✅ Triton kernels generated!</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   ❌ No Triton kernels generated</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">   ❌ No debug directories found</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">🎉 Demo completed! Check the generated debug files.</span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">=</span><span class="sh">"</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="nf">run_complete_demo</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run the complete demo:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>python complete_demo.py
</pre></td></tr></tbody></table></code></div></div>

<hr />

<h2 id="-key-takeaways"><span class="me-2">🎓 Key Takeaways</span><a href="#-key-takeaways" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-youve-learned"><span class="me-2">What You’ve Learned</span><a href="#what-youve-learned" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ol>
  <li><strong>TorchDynamo</strong>: Converts Python functions to computational graphs through sophisticated bytecode analysis and symbolic execution</li>
  <li><strong>TorchInductor</strong>: Optimizes graphs and generates backend-specific code</li>
  <li><strong>Triton</strong>: Creates highly optimized GPU kernels</li>
  <li><strong>Performance</strong>: Significant speedups through kernel fusion and memory optimization</li>
</ol>

<h3 id="interactive-commands-summary"><span class="me-2">Interactive Commands Summary</span><a href="#interactive-commands-summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="c"># Run the complete demo</span>
python complete_demo.py

<span class="c"># Explore debug files</span>
python debug_explorer.py

<span class="c"># Check for Triton kernels</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"output_code.py"</span>

<span class="c"># View generated files</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-type</span> f | <span class="nb">sort</span>
</pre></td></tr></tbody></table></code></div></div>

<h3 id="next-steps"><span class="me-2">Next Steps</span><a href="#next-steps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ol>
  <li><strong>Experiment with different functions</strong>: Try different operations to see what triggers Triton generation</li>
  <li><strong>Explore the debug files</strong>: Examine the generated code to understand the optimizations</li>
  <li><strong>Benchmark your own code</strong>: Apply <code class="language-plaintext highlighter-rouge">torch.compile</code> to your own PyTorch functions</li>
  <li><strong>Read the documentation</strong>: Dive deeper into PyTorch’s compilation system</li>
</ol>

<p>The combination of these technologies makes PyTorch’s compilation system one of the most sophisticated and effective approaches to optimizing deep learning workloads. Happy compiling! 🚀</p>

  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/machine-learning/">Machine Learning</a>,
          <a href="/categories/pytorch/">PyTorch</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/pytorch/"
            class="post-tag no-text-decoration"
          >pytorch</a>
        
          <a
            href="/tags/torch-compile/"
            class="post-tag no-text-decoration"
          >torch.compile</a>
        
          <a
            href="/tags/triton/"
            class="post-tag no-text-decoration"
          >triton</a>
        
          <a
            href="/tags/optimization/"
            class="post-tag no-text-decoration"
          >optimization</a>
        
          <a
            href="/tags/deep-learning/"
            class="post-tag no-text-decoration"
          >deep-learning</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Torch.compile%20Deep%20Dive%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F00%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Torch.compile%20Deep%20Dive%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache&u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F00%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F00%2F&text=Torch.compile%20Deep%20Dive%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/00/">Torch.compile Deep Dive - From Python Function to Triton Kernel</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/optimization/">optimization</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/torch-compile/">torch.compile</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/triton/">triton</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    











            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Older">
      <p>-</p>
    </div>
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Newer">
      <p>-</p>
    </div>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>©
    <time>2025</time>

    
      <a href="https://twitter.com/supercharles">supercharles</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.3.1"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/optimization/">optimization</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/torch-compile/">torch.compile</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/triton/">triton</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

