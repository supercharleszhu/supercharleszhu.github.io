<!doctype html>














<!-- `site.alt_lang` can specify a language different from the UI -->
<html lang="en" >
  <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e">
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta
    name="viewport"
    content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover"
  ><!-- Setup Open Graph image -->

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Torch.compile 101 - From Python Function to Triton Kernel" />
<meta name="author" content="Charles Zhu" />
<meta property="og:locale" content="en" />
<meta name="description" content="An interactive deep dive into PyTorchâ€™s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis." />
<meta property="og:description" content="An interactive deep dive into PyTorchâ€™s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis." />
<link rel="canonical" href="https://supercharles.github.io/posts/2025-08-06-00/" />
<meta property="og:url" content="https://supercharles.github.io/posts/2025-08-06-00/" />
<meta property="og:site_name" content="ZCache" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-08-06T19:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Torch.compile 101 - From Python Function to Triton Kernel" />
<meta name="twitter:site" content="@supercharles" />
<meta name="twitter:creator" content="@Charles Zhu" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Charles Zhu"},"dateModified":"2025-08-25T09:33:10-07:00","datePublished":"2025-08-06T19:00:00-07:00","description":"An interactive deep dive into PyTorchâ€™s torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.","headline":"Torch.compile 101 - From Python Function to Triton Kernel","mainEntityOfPage":{"@type":"WebPage","@id":"https://supercharles.github.io/posts/2025-08-06-00/"},"url":"https://supercharles.github.io/posts/2025-08-06-00/"}</script>
<!-- End Jekyll SEO tag -->


  <title>Torch.compile 101 - From Python Function to Triton Kernel | ZCache
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://realfavicongenerator.net/
-->



<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

  <link rel="manifest" href="/assets/img/favicons/site.webmanifest">

<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico">
<meta name="apple-mobile-web-app-title" content="ZCache">
<meta name="application-name" content="ZCache">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml">
<meta name="theme-color" content="#ffffff">


  <!-- Resource Hints -->
  
    
      
        <link rel="preconnect" href="https://fonts.googleapis.com" >
      
        <link rel="dns-prefetch" href="https://fonts.googleapis.com" >
      
    
      
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
      
        <link rel="dns-prefetch" href="https://fonts.gstatic.com" >
      
    
      
        <link rel="preconnect" href="https://cdn.jsdelivr.net" >
      
        <link rel="dns-prefetch" href="https://cdn.jsdelivr.net" >
      
    
  

  <!-- Bootstrap -->
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css">
  

  <!-- Theme style -->
  <link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css">

  <!-- Web Font -->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap">

  <!-- Font Awesome Icons -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.1/css/all.min.css">

  <!-- 3rd-party Dependencies -->

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.32.2/dist/tocbot.min.css">
  

  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css">
  

  
    <!-- Image Popup -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css">
  

  <!-- Scripts -->

  <script src="/assets/js/dist/theme.min.js"></script>

  <!-- JS selector for site. -->

<!-- commons -->



<!-- layout specified -->


  

  
    <!-- image lazy-loading & popup & clipboard -->
    
  















  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  

  
    

    

  



  <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.32.2/dist/tocbot.min.js"></script>







<script defer src="/assets/js/dist/post.min.js"></script>



<!-- Pageviews -->

  

  



  

  <!-- A placeholder to allow defining custom metadata -->

</head>


  <body>
    <!-- The Side Bar -->

<aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end">
  <header class="profile-wrapper">
    <a href="/" id="avatar" class="rounded-circle"><img src="/assets/img/favicons/avatar.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a>

    <a class="site-title d-block" href="/">ZCache</a>
    <p class="site-subtitle fst-italic mb-0">SuperCharles's Engineering Blog</p>
  </header>
  <!-- .profile-wrapper -->

  <nav class="flex-column flex-grow-1 w-100 ps-0">
    <ul class="nav">
      <!-- home -->
      <li class="nav-item">
        <a href="/" class="nav-link">
          <i class="fa-fw fas fa-home"></i>
          <span>HOME</span>
        </a>
      </li>
      <!-- the real tabs -->
      
        <li class="nav-item">
          <a href="/categories/" class="nav-link">
            <i class="fa-fw fas fa-stream"></i>
            

            <span>CATEGORIES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/tags/" class="nav-link">
            <i class="fa-fw fas fa-tags"></i>
            

            <span>TAGS</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/archives/" class="nav-link">
            <i class="fa-fw fas fa-archive"></i>
            

            <span>ARCHIVES</span>
          </a>
        </li>
        <!-- .nav-item -->
      
        <li class="nav-item">
          <a href="/about/" class="nav-link">
            <i class="fa-fw fas fa-info-circle"></i>
            

            <span>ABOUT</span>
          </a>
        </li>
        <!-- .nav-item -->
      
    </ul>
  </nav>

  <div class="sidebar-bottom d-flex flex-wrap  align-items-center w-100">
    
      <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle">
        <i class="fas fa-adjust"></i>
      </button>

      
        <span class="icon-border"></span>
      
    

    
      

      
        <a
          href="https://github.com/supercharles"
          aria-label="github"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fab fa-github"></i>
        </a>
      
    
      

      
        <a
          href="https://twitter.com/supercharles"
          aria-label="twitter"
          

          
            target="_blank"
            
          

          

          
            rel="noopener noreferrer"
          
        >
          <i class="fa-brands fa-x-twitter"></i>
        </a>
      
    
      

      
        <a
          href="javascript:location.href = 'mailto:' + ['zhuchen1033','gmail.com'].join('@')"
          aria-label="email"
          

          

          

          
        >
          <i class="fas fa-envelope"></i>
        </a>
      
    
      

      
        <a
          href="/feed.xml"
          aria-label="rss"
          

          

          

          
        >
          <i class="fas fa-rss"></i>
        </a>
      
    
  </div>
  <!-- .sidebar-bottom -->
</aside>
<!-- #sidebar -->


    <div id="main-wrapper" class="d-flex justify-content-center">
      <div class="container d-flex flex-column px-xxl-5">
        <!-- The Top Bar -->

<header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar">
  <div
    id="topbar"
    class="d-flex align-items-center justify-content-between px-lg-3 h-100"
  >
    <nav id="breadcrumb" aria-label="Breadcrumb">
      

      
        
          
            <span>
              <a href="/">Home</a>
            </span>

          
        
          
        
          
            
              <span>Torch.compile 101 - From Python Function to Triton Kernel</span>
            

          
        
      
    </nav>
    <!-- endof #breadcrumb -->

    <button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar">
      <i class="fas fa-bars fa-fw"></i>
    </button>

    <div id="topbar-title">
      Post
    </div>

    <button type="button" id="search-trigger" class="btn btn-link" aria-label="Search">
      <i class="fas fa-search fa-fw"></i>
    </button>

    <search id="search" class="align-items-center ms-3 ms-lg-0">
      <i class="fas fa-search fa-fw"></i>
      <input
        class="form-control"
        id="search-input"
        type="search"
        aria-label="search"
        autocomplete="off"
        placeholder="Search..."
      >
    </search>
    <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
  </div>
</header>


        <div class="row flex-grow-1">
          <main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              <!-- Refactor the HTML structure -->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->



<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->



<!-- Change the icon of checkbox -->



<!-- Handle images -->




  
  

  
    
      
      
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  
    

    
    

    

    
    

    
    
    

    
      

      
      
      

      
    
      

      
      
      

      
    

    <!-- take out classes -->
    

    

    
    

    

    
      
    

    <!-- lazy-load images -->
    

    
      <!-- make sure the `<img>` is wrapped by `<a>` -->
      

      
        <!-- create the image wrapper -->
        

        
        
      
    

    <!-- combine -->
    
  

  


<!-- Add header for code snippets -->



<!-- Create heading anchors -->





  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  
    
    

    
      
        
        
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    
      

      
      

      
      
      

      
    

    
  

  
  

  




<!-- return -->










<article class="px-1" data-toc="true">
  <header>
    <h1 data-toc-skip>Torch.compile 101 - From Python Function to Triton Kernel</h1>
    
      <p class="post-desc fw-light mb-4">An interactive deep dive into PyTorch's torch.compile system, tracing the journey from Python functions to optimized Triton kernels with hands-on examples and performance analysis.</p>
    

    <div class="post-meta text-muted">
      <!-- published date -->
      <span>
        Posted
        <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1754532000"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Aug  6, 2025
</time>

      </span>

      <!-- lastmod date -->
      
        <span>
          Updated
          <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1756139590"
  data-df="ll"
  
    data-bs-toggle="tooltip" data-bs-placement="bottom"
  
>
  Aug 25, 2025
</time>

        </span>
      

      

      <div class="d-flex justify-content-between">
        <!-- author(s) -->
        <span>
          

          By

          <em>
            
              
                
                
              
            
          </em>
        </span>

        <div>
          <!-- pageviews -->
          

          <!-- read time -->
          <!-- Calculate the post's reading time, and display the word count in tooltip -->



<!-- words per minute -->










<!-- return element -->
<span
  class="readtime"
  data-bs-toggle="tooltip"
  data-bs-placement="bottom"
  title="3434 words"
>
  <em>19 min</em> read</span>

        </div>
      </div>
    </div>
  </header>

  
    <div id="toc-bar" class="d-flex align-items-center justify-content-between invisible">
      <span class="label text-truncate">Torch.compile 101 - From Python Function to Triton Kernel</span>
      <button type="button" class="toc-trigger btn me-1">
        <i class="fa-solid fa-list-ul fa-fw"></i>
      </button>
    </div>

    <button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm">
      <span class="label ps-2 pe-1">Contents</span>
      <i class="fa-solid fa-angle-right fa-fw"></i>
    </button>

    <dialog id="toc-popup" class="p-0">
      <div class="header d-flex flex-row align-items-center justify-content-between">
        <div class="label text-truncate py-2 ms-4">Torch.compile 101 - From Python Function to Triton Kernel</div>
        <button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75">
          <i class="fas fa-close"></i>
        </button>
      </div>
      <div id="toc-popup-content" class="px-4 py-3 pb-4"></div>
    </dialog>
  

  <div class="content">
    <p><a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fsupercharleszhu.github.io%2Fposts%2F00%2F&amp;label=view&amp;icon=book&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fsupercharleszhu.github.io%2Fposts%2F00%2F&amp;label=view&amp;icon=book&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" alt="Badge" loading="lazy"></a></p>

<h2 id="introduction"><span class="me-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p><code class="language-plaintext highlighter-rouge">torch.compile</code> in pytorch 2 is a common and revolutionary approach to optimizing PyTorch eager training performance. As AI Infra engineer, it is common case that we need to deal with benchmarking with different compiling configuration. Dirty tuning is usually fine and sometimes necessary, but more importantly, we want to deep dive and understand the underlying mechanism (which is also critical skills we want to build as system engineer!)</p>

<p>In this interactive deep dive, weâ€™ll trace the journey of a simple Python function through the entire compilation pipeline, from the initial function definition to the final execution of optimized Triton kernels.</p>

<p><strong>ðŸŽ¯ Interactive Goal</strong>: By the end of this guide, youâ€™ll be able to run commands and see the actual compilation process in action!</p>

<p><strong>ðŸ“š Recommended Reading</strong>: This will be a dirty code walkthrough guide. If you want to understand this from theory point of view, the <a href="https://docs.pytorch.org/assets/pytorch2-2.pdf">original pytorch 2 paper</a> is the best material in my opinion. Actually, I am using this paper personally as a starter point to understand the overall picture!</p>

<h2 id="phase-1-torchdynamo---graph-capture"><span class="me-2">Phase 1: TorchDynamo - Graph Capture</span><a href="#phase-1-torchdynamo---graph-capture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-torchdynamo"><span class="me-2">What is TorchDynamo?</span><a href="#what-is-torchdynamo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TorchDynamo is PyTorchâ€™s graph capture mechanism that converts Python functions into computational graphs. It is hacking into the CPythonâ€™s compilation stage (<a href="https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html#pep-523-adding-a-frame-evaluation-api-to-cpython">reference</a>) and modify the bytecode to be actually executed.</p>

<p><a href="/assets/img/posts/dynamo.png" class="popup img-link shimmer"><img src="/assets/img/posts/dynamo.png" alt="TorchDynamo Architecture" loading="lazy"></a></p>

<h3 id="interactive-example-lets-trace-a-function"><span class="me-2">Interactive Example: Letâ€™s Trace a Function</span><a href="#interactive-example-lets-trace-a-function" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Create a file called <code class="language-plaintext highlighter-rouge">trace_example.py</code>:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td><td class="rouge-code"><pre><span class="c1"># torch version: 2.8.0
</span><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">os</span>

<span class="n">torch</span><span class="p">.</span><span class="n">compiler</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">complex_function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">A function that will trigger various optimizations</span><span class="sh">"""</span>
    <span class="c1"># Multiple operations that can be fused
</span>    <span class="n">a</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="mf">2.0</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">silu</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="c1"># this will trigger decomposition
</span>    
    <span class="c1"># Reduction operations that trigger Triton code generation
</span>    <span class="n">d</span> <span class="o">=</span> <span class="n">e</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Reduce along dimension 1
</span>    <span class="n">f</span> <span class="o">=</span> <span class="n">d</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span>       <span class="c1"># Global mean
</span>    
    <span class="k">return</span> <span class="n">f</span>

<span class="c1"># Create larger tensors to see more optimization
</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">input1</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">input2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span><span class="p">)</span>

<span class="n">compiled_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">complex_function</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="nf">compiled_fn</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">)</span>
<span class="c1"># result.backward()
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Run this example:</strong></p>
<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td class="rouge-code"><pre><span class="nb">echo</span> <span class="s2">"Setting up environment variables for PyTorch Compile debugging..."</span>
<span class="nb">export </span><span class="nv">TORCH_COMPILE_DEBUG</span><span class="o">=</span>1
<span class="nb">export </span><span class="nv">TORCH_LOGS</span><span class="o">=</span>+dynamo,+inductor

<span class="nb">echo</span> <span class="s2">"Environment variables set:"</span>
<span class="nb">echo</span> <span class="s2">"TORCH_COMPILE_DEBUG=</span><span class="nv">$TORCH_COMPILE_DEBUG</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"TORCH_LOGS=</span><span class="nv">$TORCH_LOGS</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="s2">""</span>
<span class="nb">echo</span> <span class="s2">"Running PyTorch Compile Deep Dive Demonstration..."</span>
<span class="nb">echo</span> <span class="s2">"================================================"</span>

<span class="nb">rm</span> <span class="nt">-rf</span> /tmp/torchinductor_jobuser <span class="c"># cleanup the torchinductor cache</span>

python trace_example.py

<span class="nb">echo</span> <span class="s2">""</span>
<span class="nb">echo</span> <span class="s2">"Demo completed! Check the generated debug files in torch_compile_debug/"</span> 
</pre></td></tr></tbody></table></code></div></div>

<p><strong>What youâ€™ll see (under <code class="language-plaintext highlighter-rouge">torch_compile_debug/run_xxx/torchdynamo/debug.log</code>):</strong></p>
<ul>
  <li>Detailed bytecode tracing output showing each instruction</li>
  <li>FX graph construction with node creation</li>
  <li>Guard generation for tensor shapes and types</li>
  <li>Variable tracking and symbolic execution</li>
</ul>

<p>Letâ€™s deep dive into each step separately!</p>

<h3 id="-deep-dive-torchdynamos-internal-architecture"><span class="me-2">ðŸ”¬ Deep Dive: TorchDynamoâ€™s Internal Architecture</span><a href="#-deep-dive-torchdynamos-internal-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Letâ€™s dive deep into code base and check how it is tracing the program!</p>

<h4 id="entry-point"><span class="me-2"><strong>Entry Point</strong></span><a href="#entry-point" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>An execution of a Python program roughly consists of three stages:</p>

<ol>
  <li>Initialization: handles import, etc.</li>
  <li>Compilation: run lexing, parsing, etc to convert python code into bytecode for execution.</li>
  <li>Interpretation: run CPython VM to execute the python function</li>
</ol>

<p>The dynamo analysis was actually hacking into compilation stage, and rewrite the bytecode to be executed. When there is no already compiled function, we start a fresh analysis and tracing from scratch.</p>

<p>First step, it is using <code class="language-plaintext highlighter-rouge">dis</code> module to get the bytecode to be executed by CPython and run some initial cleanup of the Bytecode. You can check compiler <a href="https://github.com/python/cpython/blob/main/InternalDocs/compiler.md">here</a> on details of how it is generating the bytecode. Also <a href="https://tenthousandmeters.com/blog/python-behind-the-scenes-1-how-the-cpython-vm-works/">here</a> is a great blog sharing how CPython is compiling and interpreting a python program</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="c1"># torch version: 2.8.0+cu126
</span>
<span class="c1"># From bytecode_transformation.py
</span><span class="k">def</span> <span class="nf">cleaned_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">,</span> <span class="n">safe</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Instruction</span><span class="p">]:</span>
    <span class="n">instructions</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="n">convert_instruction</span><span class="p">,</span> <span class="n">dis</span><span class="p">.</span><span class="nf">get_instructions</span><span class="p">(</span><span class="n">code</span><span class="p">)))</span>
    <span class="c1"># rest part are omitted
</span></pre></td></tr></tbody></table></code></div></div>

<h4 id="symbolic-execution-engine"><span class="me-2"><strong>Symbolic Execution Engine</strong></span><a href="#symbolic-execution-engine" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Instead of executing the bytecode via CPython interpreter, we created our own tracer to trace it. The core tracing happens in <code class="language-plaintext highlighter-rouge">symbolic_convert.py</code> through the <code class="language-plaintext highlighter-rouge">InstructionTranslator</code> class:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">class</span> <span class="nc">InstructionTranslatorBase</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
        <span class="c1">#...
</span>        <span class="n">output_graph</span> <span class="o">=</span> <span class="nc">OutputGraph</span><span class="p">(...)</span> <span class="c1"># storing the fx graph, do code generation
</span>        <span class="c1">#...
</span>        <span class="n">var</span> <span class="o">=</span> <span class="n">LazyVariableTracker</span><span class="p">.</span><span class="nf">create</span><span class="p">(...)</span> <span class="c1"># create var tracker for input Tensors
</span>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Process exactly one instruction, return False we should exit</span><span class="sh">"""</span>
        <span class="n">ip</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">instruction_pointer</span>
        <span class="k">if</span> <span class="n">ip</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_instruction</span> <span class="o">=</span> <span class="n">inst</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">instructions</span><span class="p">[</span><span class="n">ip</span><span class="p">]</span>
        <span class="n">self</span><span class="p">.</span><span class="n">instruction_pointer</span> <span class="o">=</span> <span class="n">ip</span> <span class="o">+</span> <span class="mi">1</span>
        
        <span class="c1"># Dispatch to appropriate handler
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dispatch_table</span><span class="p">[</span><span class="n">inst</span><span class="p">.</span><span class="n">opcode</span><span class="p">](</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p><strong>Instruction-by-Instruction Processing:</strong></p>
<ul>
  <li>Each Python bytecode instruction has a corresponding handler</li>
  <li>Variables are tracked as <code class="language-plaintext highlighter-rouge">VariableTracker</code> objects</li>
  <li>Stack management maintains symbolic state</li>
</ul>

<h4 id="bytecode-instruction-handlers"><span class="me-2"><strong>Bytecode Instruction Handlers</strong></span><a href="#bytecode-instruction-handlers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>Each Python bytecode instruction has a specialized handler to generate output graph</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">def</span> <span class="nf">LOAD_FAST</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle loading local variables</span><span class="sh">"""</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_load_fast</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">STORE_FAST</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle storing local variables</span><span class="sh">"""</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_store_fast</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">CALL_FUNCTION</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle function calls</span><span class="sh">"""</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">popn</span><span class="p">(</span><span class="n">inst</span><span class="p">.</span><span class="n">argval</span><span class="p">)</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pop</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">call_function</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="p">{})</span>

<span class="k">def</span> <span class="nf">BINARY_ADD</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Handle addition operations</span><span class="sh">"""</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">popn</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">push</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">call_method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="sh">"</span><span class="s">__add__</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="p">{}))</span>
</pre></td></tr></tbody></table></code></div></div>
<h4 id="variabletracker"><span class="me-2">VariableTracker</span><a href="#variabletracker" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p><a href="https://github.com/pytorch/pytorch/blob/1de4540449ad6b9df8f452ab72da30ce8908af60/torch/_dynamo/variables/base.py#L235">VariableTracker</a> is the base class for tracked locals and stack values</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">VariableTracker</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">call_function</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">tx</span><span class="p">:</span> <span class="sh">"</span><span class="s">InstructionTranslator</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">args</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="sh">"</span><span class="s">VariableTracker</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">kwargs</span><span class="p">:</span> <span class="sh">"</span><span class="s">dict[str, VariableTracker]</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">VariableTracker</span><span class="sh">"</span><span class="p">:</span>
        <span class="c1"># subclass is supposed to implement this ex. for "add" operation, it will be builtin functional variable tracker to insert and operation into graph located in Instruction Translator
</span>        <span class="c1"># ...
</span>     <span class="k">def</span> <span class="nf">call_method</span><span class="p">(</span>
        <span class="bp">...</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="sh">"</span><span class="s">VariableTracker</span><span class="sh">"</span><span class="p">:</span>
        <span class="c1"># handles method call for certain variable, ex. tensor.item; tensor[:, 1]
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="bp">...</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">source</span> <span class="c1"># source for guard creation (ex. input of the graph)
</span></pre></td></tr></tbody></table></code></div></div>
<h4 id="outputgraph"><span class="me-2">OutputGraph</span><a href="#outputgraph" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p><a href="https://github.com/pytorch/pytorch/blob/726dce3c944cbda16e54d3b15cdb4b6ced05af72/torch/_dynamo/output_graph.py#L378">OutputGraph</a> is class located in InstructionTranslator to manage the generated FX graph. When FX graph is traced, it will call <code class="language-plaintext highlighter-rouge">compiles_and_call_fx_graph</code> to further compile it into hardware-specific executables (ex. Torchinductor generating cuda kernels)</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># simplified code
</span><span class="k">class</span> <span class="nc">OutputGraph</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">():</span>
        <span class="n">self</span><span class="p">.</span><span class="n">export</span> <span class="o">=</span> <span class="n">export</span> <span class="c1"># whether to run in torch export (one graph) mode
</span>        <span class="n">self</span><span class="p">.</span><span class="n">compiler_fn</span> <span class="o">=</span> <span class="n">compiler_fn</span> <span class="c1"># backend compiler ex. TorchInductor
</span>        <span class="n">self</span><span class="p">.</span><span class="n">root_tx</span> <span class="o">=</span> <span class="n">root_tx</span> <span class="c1"># InstructionTranslator
</span>    <span class="k">def</span> <span class="nf">compile_and_call_fx_graph</span><span class="p">():</span>
        <span class="c1">#Generate code from self.graph and return the Instruction()s to
</span>        <span class="c1"># call that generated code.
</span></pre></td></tr></tbody></table></code></div></div>
<h4 id="guard-generation"><span class="me-2"><strong>Guard Generation</strong></span><a href="#guard-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>When building up the graph, the VariableTracker generates comprehensive <a href="https://github.com/pytorch/pytorch/blob/ed77e23b6808065c927e626f471d5bf588dd2cc2/torch/_guards.py#L246">guards</a> to ensure correctness:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="rouge-code"><pre><span class="c1"># From guards.py
</span><span class="k">def</span> <span class="nf">install_guard</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">guard_fn</span><span class="p">):</span>
    <span class="c1"># Install guards for tensor shapes, types, and values
</span>    <span class="c1"># Guards ensure graph validity across different inputs
</span></pre></td></tr></tbody></table></code></div></div>

<p><strong>Guard Types:</strong></p>
<ul>
  <li><strong>Shape Guards</strong>: Ensure tensor dimensions match</li>
  <li><strong>Type Guards</strong>: Ensure tensor dtypes are consistent</li>
  <li><strong>Value Guards</strong>: Ensure constant values match</li>
  <li><strong>Global Guards</strong>: Track changes to global state</li>
</ul>

<h4 id="graph-break-handling"><span class="me-2"><strong>Graph Break Handling</strong></span><a href="#graph-break-handling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>When TorchDynamo encounters unsupported operations, it creates graph breaks:</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># From symbolic_convert.py
</span><span class="k">def</span> <span class="nf">break_graph_if_unsupported</span><span class="p">(</span><span class="o">*</span><span class="p">,</span> <span class="n">push</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">decorator</span><span class="p">(</span><span class="n">inner_fn</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="k">return</span> <span class="nf">inner_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">inst</span><span class="p">)</span>
            <span class="k">except</span> <span class="n">Unsupported</span><span class="p">:</span>
                <span class="c1"># Create graph break and restart analysis
</span>                <span class="n">self</span><span class="p">.</span><span class="n">current_speculation</span><span class="p">.</span><span class="nf">fail_and_restart_analysis</span><span class="p">()</span>
</pre></td></tr></tbody></table></code></div></div>

<h3 id="-torch-dynamo-summary"><span class="me-2">ðŸŽ¯ Torch Dynamo Summary</span><a href="#-torch-dynamo-summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>So far we have talked about overview of overall steps: byte-code execution, graph construction, graph-breaks, and guards. But the actual Torchdynamo is far more complicated than this overview (ex. important part is how AOTAutograd will work when we are tracing forward and backward pass, how do we handle mutations + functionalization, etc.) We will introduce some details on-demand in the following up series.</p>

<p>We now look into the phase 2 for hardware-specific optimization - TorchInductor</p>

<h2 id="phase-2-torchinductor---graph-optimization"><span class="me-2">Phase 2: TorchInductor - Graph Optimization</span><a href="#phase-2-torchinductor---graph-optimization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-is-torchinductor"><span class="me-2">What is TorchInductor?</span><a href="#what-is-torchinductor" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>TorchInductor is PyTorchâ€™s backend compiler that takes the FX graph from TorchDynamo and optimizes it for execution on various backends (CPU, CUDA, etc.). For CUDA, it generates Triton kernels.</p>

<h3 id="what-is-triton"><span class="me-2">What is Triton?</span><a href="#what-is-triton" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>Triton is a language and compiler for writing highly optimized GPU kernels. It provides:</p>
<ul>
  <li><strong>High-level abstractions</strong> for GPU programming</li>
  <li><strong>Automatic optimization</strong> of memory access patterns</li>
  <li><strong>Parallel execution</strong> primitives</li>
  <li><strong>Integration</strong> with PyTorch</li>
</ul>

<h3 id="interactive-example-see-the-generation-process"><span class="me-2">Interactive Example: See the Generation Process</span><a href="#interactive-example-see-the-generation-process" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<p>After running the example in phase 1, check the debug directory:</p>

<div class="language-bash highlighter-rouge"><div class="code-header">
        <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="c"># Find the latest debug directory</span>
find torch_compile_debug/run_<span class="k">*</span>/torchinductor/<span class="k">*</span>/ <span class="nt">-name</span> <span class="s2">"*.py"</span> <span class="nt">-o</span> <span class="nt">-name</span> <span class="s2">"*.txt"</span> | <span class="nb">head</span> <span class="nt">-10</span>
</pre></td></tr></tbody></table></code></div></div>

<p>This will show you the generated files including:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">output_code.py</code>: Generated Triton kernels</li>
  <li><code class="language-plaintext highlighter-rouge">ir_pre_fusion.txt</code>: IR before fusion</li>
  <li><code class="language-plaintext highlighter-rouge">ir_post_fusion.txt</code>: IR after fusion</li>
</ul>

<hr />

<h3 id="-deep-dive-torchinductors-triton-code-generation"><span class="me-2">ðŸ”¬ Deep Dive: TorchInductorâ€™s Triton Code Generation</span><a href="#-deep-dive-torchinductors-triton-code-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<h4 id="entry-point-1"><span class="me-2">Entry Point</span><a href="#entry-point-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>First of all, when TorchDynamo has finished tracing the fx graph and reached the returning statement, it will trigger a <code class="language-plaintext highlighter-rouge">compile_subgraph</code> for further optimization</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_dynamo/symbolic_convert.py
# pytorch version: 2.8.0
</span><span class="k">class</span> <span class="nc">InstructionTranslator</span><span class="p">:</span>
    <span class="c1">#....
</span>    <span class="k">def</span> <span class="nf">_return</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inst</span><span class="p">):</span>
        <span class="c1">#...
</span>        <span class="n">all_stack_locals_metadata</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">output</span><span class="p">.</span><span class="nf">compile_subgraph</span><span class="p">(</span>
            <span class="n">self</span><span class="p">,</span>
            <span class="n">reason</span><span class="o">=</span><span class="nc">GraphCompileReason</span><span class="p">(</span>
                <span class="sh">"</span><span class="s">return_value</span><span class="sh">"</span><span class="p">,</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">frame_summary</span><span class="p">()],</span> <span class="n">graph_break</span><span class="o">=</span><span class="bp">False</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># ...
</span></pre></td></tr></tbody></table></code></div></div>

<p>Then it goes into the entry point of inductor backend to compile the fx graph:</p>
<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_inductor/compile_fx.py
# pytorch version: 2.8.0
</span><span class="k">def</span> <span class="nf">compile_fx</span><span class="p">(</span>
    <span class="n">model_</span><span class="p">:</span> <span class="n">GraphModule</span><span class="p">,</span>
    <span class="n">example_inputs_</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">InputType</span><span class="p">],</span>
    <span class="n">inner_compile</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[...,</span> <span class="n">OutputCode</span><span class="p">]</span> <span class="o">=</span> <span class="n">compile_fx_inner</span><span class="p">,</span>
    <span class="n">config_patches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">decompositions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="n">OpOverload</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[...,</span> <span class="n">Any</span><span class="p">]]]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="n">ignore_shape_env</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">list</span><span class="p">[</span><span class="nb">object</span><span class="p">]],</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">Weights</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Main entry point for compiling given FX graph.  Despite the fact that this
    lives in :mod:`torch._inductor`, this function is responsible for calling
    into AOT Autograd (and we will eventually get a callback to
    ``inner_compile`` to perform actual compilation.  In other words, this
    function orchestrates end-to-end compilation for the inductor backend when
    you use :func:`torch.compile`.

    NB: This function TAKES OWNERSHIP of the input ``model_`` and can potentially
    mutate it!  Make a copy if you need to preserve the original GraphModule.
    </span><span class="sh">"""</span>
</pre></td></tr></tbody></table></code></div></div>
<h4 id="aotautograd"><span class="me-2">AOTAutograd</span><a href="#aotautograd" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>

<p>The <code class="language-plaintext highlighter-rouge">AOTAutograd</code> will basically help capturing backward graph if there are inputs that requires to capture gradient ahead of time.</p>

<p>Note: This module is not only embedded within Torchinductor, it is also used by torch.export, and can be the entry point of <a href="https://docs.pytorch.org/docs/stable/torch.compiler_custom_backends.html#custom-backends-after-aotautograd">custom compiler backend</a>.</p>

<p>See <a href="https://dev-discuss.pytorch.org/t/how-does-torch-compile-work-with-autograd/1621/2">how does autograd work for torch.compile</a> for high-level ideas.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_inductor/compile_fx.py
# torch version: 2.8.0
</span><span class="k">def</span> <span class="nf">compile_fx</span><span class="p">(...):</span>
        <span class="k">return</span> <span class="nf">aot_autograd</span><span class="p">(</span>
                    <span class="n">fw_compiler</span><span class="o">=</span><span class="n">fw_compiler</span><span class="p">,</span>
                    <span class="n">bw_compiler</span><span class="o">=</span><span class="n">bw_compiler</span><span class="p">,</span>
                    <span class="n">inference_compiler</span><span class="o">=</span><span class="n">inference_compiler</span><span class="p">,</span>
                    <span class="n">decompositions</span><span class="o">=</span><span class="n">decompositions</span><span class="p">,</span>
                    <span class="n">partition_fn</span><span class="o">=</span><span class="n">partition_fn</span><span class="p">,</span>
                    <span class="n">keep_inference_input_mutations</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                    <span class="n">cudagraphs</span><span class="o">=</span><span class="n">cudagraphs</span><span class="p">,</span>
                    <span class="n">boxed_forward_device_index</span><span class="o">=</span><span class="n">forward_device</span><span class="p">,</span>
                    <span class="n">ignore_shape_env</span><span class="o">=</span><span class="n">ignore_shape_env</span><span class="p">,</span>
                <span class="p">)(</span><span class="n">model_</span><span class="p">,</span> <span class="n">example_inputs_</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p>it will dispatch to the <code class="language-plaintext highlighter-rouge">aot_dispatch_autograd</code> function:</p>
<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py
# torch version: 2.8.0
</span><span class="k">def</span> <span class="nf">aot_dispatch_autograd</span><span class="p">(</span>
    <span class="n">flat_fn</span><span class="p">,</span>
    <span class="n">flat_args</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
    <span class="n">aot_config</span><span class="p">:</span> <span class="n">AOTConfig</span><span class="p">,</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="n">fw_metadata</span><span class="p">:</span> <span class="n">ViewAndMutationMeta</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DispatchReturn</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Autograd logic. Generates a joint graph, partitions it, manipulates the input with various wrappers,
    and returns a wrapped torch.autograd.Function with a forward and backward.
    </span><span class="sh">"""</span>
    <span class="c1">#...
</span>    <span class="k">with</span> <span class="nf">dynamo_timed</span><span class="p">(</span><span class="sh">"</span><span class="s">aot_trace_joint_graph</span><span class="sh">"</span><span class="p">,</span> <span class="n">log_pt2_compile_event</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="n">fx_g</span><span class="p">,</span> <span class="n">joint_inputs</span><span class="p">,</span> <span class="n">maybe_subclass_meta</span> <span class="o">=</span> <span class="nf">aot_dispatch_autograd_graph</span><span class="p">(</span>
            <span class="n">flat_fn</span><span class="p">,</span> <span class="n">flat_args</span><span class="p">,</span> <span class="n">aot_config</span><span class="p">,</span> <span class="n">fw_metadata</span><span class="o">=</span><span class="n">fw_metadata</span>
        <span class="p">)</span> <span class="c1"># create joint graph
</span>    <span class="c1">#...
</span>    <span class="n">fw_module</span><span class="p">,</span> <span class="n">bw_module</span> <span class="o">=</span> <span class="n">aot_config</span><span class="p">.</span><span class="nf">partition_fn</span><span class="p">(</span>
                <span class="n">fx_g</span><span class="p">,</span>
                <span class="n">joint_inputs</span><span class="p">,</span>
                <span class="n">num_fwd_outputs</span><span class="o">=</span><span class="n">num_inner_fwd_outputs</span><span class="p">,</span>
                <span class="n">static_lifetime_input_indices</span><span class="o">=</span><span class="n">static_lifetime_input_indices</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="c1">#...
</span>    <span class="n">compiled_fn</span> <span class="o">=</span> <span class="nf">post_compile</span><span class="p">(</span>
        <span class="n">wrappers</span><span class="p">,</span>
        <span class="n">compiled_fn</span><span class="p">,</span>
        <span class="n">aot_config</span><span class="p">,</span>
        <span class="n">runtime_metadata</span><span class="o">=</span><span class="n">fw_metadata</span><span class="p">,</span>
    <span class="p">)</span> <span class="c1"># run postprocessing to wrap the input and output of the graph
</span>
    <span class="k">return</span> <span class="n">compiled_fn</span> <span class="c1"># return a torch.autograd.Function
</span>
</pre></td></tr></tbody></table></code></div></div>

<p>Note 1: when generating the joint graph, internally it is calling <code class="language-plaintext highlighter-rouge">aot_dispatch_autograd_graph</code></p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">aot_dispatch_autograd_graph</span><span class="p">(..):</span>
    <span class="c1">#...
</span>    <span class="n">joint_fn_to_trace</span><span class="p">,</span> <span class="n">updated_joint_inputs</span> <span class="o">=</span> <span class="nf">create_functionalized_fn</span><span class="p">(</span>
        <span class="bp">...</span>
    <span class="p">)</span> <span class="c1"># functionalization to avoid mutation in the graph
</span>    <span class="c1">#...
</span>    <span class="n">fx_g</span> <span class="o">=</span> <span class="nf">_create_graph</span><span class="p">(</span><span class="n">joint_fn_to_trace</span><span class="p">,</span> <span class="n">updated_joint_inputs</span><span class="p">,</span> <span class="n">aot_config</span><span class="o">=</span><span class="n">aot_config</span><span class="p">)</span> <span class="c1"># this will create the joint graph, and will run "decomposition" to lower into a smaller set of supported operation
</span>    


</pre></td></tr></tbody></table></code></div></div>
<p>Note 2: the â€œ<code class="language-plaintext highlighter-rouge">decomposition</code>â€ is run in AOTAutograd as well, while all of the decomposition is registered in <code class="language-plaintext highlighter-rouge">torch/_decom/decomposition.py</code> (we should be able to register our own decomposition as well):</p>
<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_decomp/decomposition.py
# all of the decompositions are registered under this file
</span>
<span class="nd">@register_decomposition</span><span class="p">(</span><span class="n">aten</span><span class="p">.</span><span class="n">silu</span><span class="p">)</span>
<span class="nd">@out_wrapper</span><span class="p">()</span>
<span class="nd">@pw_cast_for_opmath</span>
<span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="n">self</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">self</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">self</span><span class="p">)</span>

</pre></td></tr></tbody></table></code></div></div>

<h4 id="defined-by-run-ir"><span class="me-2">Defined-by-run IR</span><a href="#defined-by-run-ir" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>After that, it will generate a <code class="language-plaintext highlighter-rouge">lowered defined-by-run IR</code> for TorchInductor. These IR will have the layout
information to execute the graph</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_inductor/compile_fx.py
</span><span class="k">class</span> <span class="nc">_InProcessFxCompile</span><span class="p">(</span><span class="n">FxCompile</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">codegen_and_compile</span><span class="p">():</span>
        <span class="c1">#...
</span>        <span class="n">graph</span> <span class="o">=</span> <span class="nc">GraphLowering</span><span class="p">(...)</span> <span class="c1"># this will generate the defined-by-run IR
</span></pre></td></tr></tbody></table></code></div></div>

<p>We can see the generated defined-by-run graph from <code class="language-plaintext highlighter-rouge">ir_pre_fusion.txt</code></p>
<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
</pre></td><td class="rouge-code"><pre>op0: SchedulerNode(ComputedBuffer)
op0.writes = [MemoryDep('buf0', c0, {c0: 1000})]
op0.unmet_dependencies = []
op0.met_dependencies = [MemoryDep('arg0_1', c0, {c0: 1000000}), MemoryDep('arg1_1', c0, {c0: 1000000})]
op0.outputs = [
    buf0: ComputedBuffer
    buf0.layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
    buf0.users = [NodeUser(node=SchedulerNode(name='op1'), can_inplace=False, is_weak=False)]
]
op0.group.device = cuda:0
op0.group.iteration = (1000, 1000)
op0.sizes = ([1000], [1000])
arg0_1_layout = FixedLayout('cuda:0', torch.float32, size=[1000, 1000], stride=[1000, 1])
arg1_1_layout = FixedLayout('cuda:0', torch.float32, size=[1000, 1000], stride=[1000, 1])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
class op0_loop_body:
    var_ranges = {p0: 1000, p1: 1000}
    index0 = 1000*p0 + p1
    index1 = p0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('arg0_1', get_index)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('arg1_1', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(2.0, torch.float32)
        mul = ops.mul(add, constant)
        relu = ops.relu(mul)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', relu)
        get_index_2 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf0', get_index_2, reduction)
        return store_reduction


op1: SchedulerNode(ComputedBuffer)
op1.writes = [MemoryDep('buf1', 0, {})]
op1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 1000})]
op1.met_dependencies = []
op1.outputs = [
    buf1: ComputedBuffer
    buf1.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf1.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]
]
op1.group.device = cuda:0
op1.group.iteration = (1, 1000)
op1.sizes = ([], [1000])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
buf1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
class op1_loop_body:
    var_ranges = {p0: 1000}
    index0 = p0
    index1 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf0', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf1', get_index_1, reduction)
        return store_reduction


op2: SchedulerNode(ComputedBuffer)
op2.writes = [MemoryDep('buf2', 0, {})]
op2.unmet_dependencies = [MemoryDep('buf1', 0, {})]
op2.met_dependencies = []
op2.outputs = [
    buf2: ComputedBuffer
    buf2.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf2.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op2.group.device = cuda:0
op2.group.iteration = (1, 1)
op2.sizes = ([], [])
buf1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf2_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
class op2_loop_body:
    var_ranges = {}
    index0 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        constant = ops.constant(1000.0, torch.float32)
        truediv = ops.truediv(load, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf2', get_index_1, truediv, None)
        return store
</pre></td></tr></tbody></table></code></div></div>

<h4 id="scheduler"><span class="me-2">Scheduler</span><a href="#scheduler" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>The next step is fusing the kernel via <code class="language-plaintext highlighter-rouge">Scheduler</code>, which is in charge of determining whether the op should be fused or not.</p>

<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_inductor/scheduler.py
</span>
<span class="k">class</span> <span class="nc">Scheduler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(...):</span>
        <span class="c1"># ...
</span>        <span class="nf">log_ir_pre_fusion</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_orig_nodes</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">create_foreach_nodes</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">topological_sort_schedule</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">logged_slow_fusion</span> <span class="o">=</span> <span class="n">OrderedSet</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]()</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">_pre_fusion_custom_pass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="nf">_pre_fusion_custom_pass</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fuse_nodes</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">_post_fusion_custom_pass</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="nf">_post_fusion_custom_pass</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">merge_loops</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">finalize_multi_template_buffers</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">combo_kernels</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">create_combo_kernel_nodes</span><span class="p">(</span><span class="n">num_ck_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>

        <span class="c1"># Peak memory pass and overlap pass must run last, otherwise
</span>        <span class="c1"># other reordering passes could undo their effects.
</span>        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">reorder_for_peak_memory</span><span class="p">:</span>
            <span class="kn">from</span> <span class="n">.memory</span> <span class="kn">import</span> <span class="n">reorder_for_peak_memory</span>

            <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="nf">reorder_for_peak_memory</span><span class="p">(</span>
                <span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">,</span>
                <span class="n">self</span><span class="p">.</span><span class="n">name_to_buf</span><span class="p">,</span>
                <span class="n">self</span><span class="p">.</span><span class="n">name_to_fused_node</span><span class="p">,</span>
                <span class="nc">OrderedSet</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="n">graph_inputs</span><span class="p">.</span><span class="nf">keys</span><span class="p">()),</span>
                <span class="nc">OrderedSet</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="nf">get_output_names</span><span class="p">()),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">reorder_for_compute_comm_overlap</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">comms</span><span class="p">.</span><span class="nf">reorder_compute_and_comm_for_overlap</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">process_grouped_nodes</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">_inductor</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">graph_partition</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">maybe_reorder_for_minimizing_partition</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">nodes</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reorder_for_partition_with_simple_dependency</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">compute_last_usage</span><span class="p">()</span>
        <span class="nf">log_ir_post_fusion</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">nodes</span><span class="p">)</span>
        <span class="c1"># ...
</span>
</pre></td></tr></tbody></table></code></div></div>

<p>We can see the post-fusion graph from <code class="language-plaintext highlighter-rouge">ir_post_fusion.txt</code></p>
<div class="language-plaintext highlighter-rouge"><div class="code-header">
        <span data-label-text="Plaintext"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
</pre></td><td class="rouge-code"><pre>#...
op1_op2: FusedSchedulerNode(SchedulerNode,SchedulerNode)
op1_op2.writes = [MemoryDep('buf1', 0, {}), MemoryDep('buf2', 0, {})]
op1_op2.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 1000})]
op1_op2.met_dependencies = []
op1_op2.outputs = [
    buf1: ComputedBuffer
    buf1.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf1.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]
    buf2: ComputedBuffer
    buf2.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf2.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op1_op2.snodes[0] =
op1: SchedulerNode(ComputedBuffer)
op1.writes = [MemoryDep('buf1', 0, {})]
op1.unmet_dependencies = [MemoryDep('buf0', c0, {c0: 1000})]
op1.met_dependencies = []
op1.outputs = [
    buf1: ComputedBuffer
    buf1.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf1.users = [NodeUser(node=SchedulerNode(name='op2'), can_inplace=True, is_weak=False)]
]
op1.group.device = cuda:0
op1.group.iteration = (1, 1000)
op1.sizes = ([], [1000])
buf0_layout = FixedLayout('cuda:0', torch.float32, size=[1000], stride=[1])
buf1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
class op1_loop_body:
    var_ranges = {p0: 1000}
    index0 = p0
    index1 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf0', get_index)
        reduction = ops.reduction(torch.float32, torch.float32, 'sum', load)
        get_index_1 = self.get_index('index1')
        store_reduction = ops.store_reduction('buf1', get_index_1, reduction)
        return store_reduction
op1_op2.snodes[1] =
op2: SchedulerNode(ComputedBuffer)
op2.writes = [MemoryDep('buf2', 0, {})]
op2.unmet_dependencies = [MemoryDep('buf1', 0, {})]
op2.met_dependencies = []
op2.outputs = [
    buf2: ComputedBuffer
    buf2.layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
    buf2.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
]
op2.group.device = cuda:0
op2.group.iteration = (1, 1)
op2.sizes = ([], [])
buf1_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
buf2_layout = FixedLayout('cuda:0', torch.float32, size=[], stride=[])
class op2_loop_body:
    var_ranges = {}
    index0 = 0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        constant = ops.constant(1000.0, torch.float32)
        truediv = ops.truediv(load, constant)
        get_index_1 = self.get_index('index0')
        store = ops.store('buf2', get_index_1, truediv, None)
        return store
</pre></td></tr></tbody></table></code></div></div>

<h4 id="triton-codegen"><span class="me-2">Triton Codegen</span><a href="#triton-codegen" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4>
<p>The last step will be the triton codegen</p>
<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="rouge-code"><pre><span class="c1"># in torch/_inductor/codegen/simd.py
</span><span class="k">class</span> <span class="nc">SIMDScheduling</span><span class="p">(</span><span class="n">BaseScheduling</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">codegen_node</span><span class="p">(</span>
            <span class="n">self</span><span class="p">,</span> <span class="n">node</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">scheduler</span><span class="p">.</span><span class="n">FusedSchedulerNode</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">.</span><span class="n">SchedulerNode</span><span class="p">]</span>
        <span class="p">):</span><span class="err">`</span>
        <span class="sh">"""</span><span class="s">
        Given a set of pre-fused nodes, generate a Triton kernel.
        </span><span class="sh">"""</span>

        <span class="n">nodes</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">scheduler</span><span class="p">.</span><span class="n">SchedulerNode</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="nf">get_nodes</span><span class="p">()</span>  <span class="c1"># type: ignore[assignment]
</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">_inductor</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">triton</span><span class="p">.</span><span class="n">coalesce_tiling_analysis</span><span class="p">:</span>
            <span class="n">coalesce_analysis</span> <span class="o">=</span> <span class="nf">analyze_memory_coalescing</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coalesce_analysis</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">numel</span><span class="p">,</span> <span class="n">rnumel</span><span class="p">)</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">int</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">is_reduction</span><span class="p">())).</span><span class="n">group</span>

        <span class="n">node_schedule</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">generate_node_schedule</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">rnumel</span><span class="p">)</span>
        <span class="n">schedule_log</span><span class="p">.</span><span class="nf">debug</span><span class="p">(</span><span class="sh">"</span><span class="s">Schedule:</span><span class="se">\n</span><span class="s"> %s</span><span class="sh">"</span><span class="p">,</span> <span class="n">node_schedule</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">codegen_node_schedule</span><span class="p">(</span>
            <span class="nc">SIMDKernelFeatures</span><span class="p">(</span><span class="n">node_schedule</span><span class="p">,</span> <span class="n">numel</span><span class="p">,</span> <span class="n">rnumel</span><span class="p">,</span> <span class="n">coalesce_analysis</span><span class="p">)</span>
        <span class="p">)</span>
</pre></td></tr></tbody></table></code></div></div>

<p>We will be able to see the final generated triton code in output_code.py</p>
<div class="language-python highlighter-rouge"><div class="code-header">
        <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span>
      <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="rouge-code"><pre><span class="c1">#...
</span><span class="nd">@triton_heuristics.persistent_reduction</span><span class="p">(</span>
    <span class="n">size_hints</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span> <span class="sh">'</span><span class="s">r0_</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1024</span><span class="p">},</span>
    <span class="n">reduction_hint</span><span class="o">=</span><span class="n">ReductionHint</span><span class="p">.</span><span class="n">INNER</span><span class="p">,</span>
    <span class="n">filename</span><span class="o">=</span><span class="n">__file__</span><span class="p">,</span>
    <span class="n">triton_meta</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">signature</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">in_ptr0</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">*fp32</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">in_ptr1</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">*fp32</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">out_ptr0</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">*fp32</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">xnumel</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">i32</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">r0_numel</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">i32</span><span class="sh">'</span><span class="p">},</span> <span class="sh">'</span><span class="s">device</span><span class="sh">'</span><span class="p">:</span> <span class="nc">DeviceProperties</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">multi_processor_count</span><span class="o">=</span><span class="mi">132</span><span class="p">,</span> <span class="n">cc</span><span class="o">=</span><span class="mi">90</span><span class="p">,</span> <span class="n">major</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">regs_per_multiprocessor</span><span class="o">=</span><span class="mi">65536</span><span class="p">,</span> <span class="n">max_threads_per_multi_processor</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">warp_size</span><span class="o">=</span><span class="mi">32</span><span class="p">),</span> <span class="sh">'</span><span class="s">constants</span><span class="sh">'</span><span class="p">:</span> <span class="p">{},</span> <span class="sh">'</span><span class="s">configs</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="nc">AttrsDescriptor</span><span class="p">(</span><span class="n">divisible_by_16</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">equal_to_1</span><span class="o">=</span><span class="p">())]},</span>
    <span class="n">inductor_meta</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">grid_type</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Grid1D</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">autotune_hints</span><span class="sh">'</span><span class="p">:</span> <span class="nf">set</span><span class="p">(),</span> <span class="sh">'</span><span class="s">kernel_name</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">triton_per_fused_add_mul_relu_silu_sum_0</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">mutated_arg_names</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span> <span class="sh">'</span><span class="s">optimize_mem</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">no_x_dim</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">num_load</span><span class="sh">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">'</span><span class="s">num_reduction</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">backend_hash</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">F0EE7D6F735E9CCA02E1254E65809C6B6C17DFB7C86025DA2A9F7ED74AE9DFB2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">are_deterministic_algorithms_enabled</span><span class="sh">'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">'</span><span class="s">assert_indirect_indexing</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">autotune_local_cache</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">autotune_pointwise</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">autotune_remote_cache</span><span class="sh">'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">'</span><span class="s">force_disable_caches</span><span class="sh">'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">'</span><span class="s">dynamic_scale_rblock</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_autotune</span><span class="sh">'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">'</span><span class="s">max_autotune_pointwise</span><span class="sh">'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">'</span><span class="s">min_split_scan_rblock</span><span class="sh">'</span><span class="p">:</span> <span class="mi">256</span><span class="p">,</span> <span class="sh">'</span><span class="s">spill_threshold</span><span class="sh">'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="sh">'</span><span class="s">store_cubin</span><span class="sh">'</span><span class="p">:</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">'</span><span class="s">tiling_scores</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="mi">8000</span><span class="p">,</span> <span class="sh">'</span><span class="s">r0_</span><span class="sh">'</span><span class="p">:</span> <span class="mi">8000000</span><span class="p">}}</span>
<span class="p">)</span>
<span class="nd">@triton.jit</span>
<span class="k">def</span> <span class="nf">triton_per_fused_add_mul_relu_silu_sum_0</span><span class="p">(</span><span class="n">in_ptr0</span><span class="p">,</span> <span class="n">in_ptr1</span><span class="p">,</span> <span class="n">out_ptr0</span><span class="p">,</span> <span class="n">xnumel</span><span class="p">,</span> <span class="n">r0_numel</span><span class="p">):</span>
    <span class="n">xnumel</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">XBLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">r0_numel</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">R0_BLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="n">rnumel</span> <span class="o">=</span> <span class="n">r0_numel</span>
    <span class="n">RBLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="n">R0_BLOCK</span>
    <span class="n">xoffset</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">XBLOCK</span>
    <span class="n">xindex</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">full</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">xoffset</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">xmask</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">full</span><span class="p">([</span><span class="n">R0_BLOCK</span><span class="p">],</span> <span class="bp">True</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="n">int1</span><span class="p">)</span>
    <span class="n">r0_index</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">R0_BLOCK</span><span class="p">)[:]</span>
    <span class="n">r0_offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">r0_mask</span> <span class="o">=</span> <span class="n">r0_index</span> <span class="o">&lt;</span> <span class="n">r0_numel</span>
    <span class="n">roffset</span> <span class="o">=</span> <span class="n">r0_offset</span>
    <span class="n">rindex</span> <span class="o">=</span> <span class="n">r0_index</span>
    <span class="n">r0_1</span> <span class="o">=</span> <span class="n">r0_index</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">xindex</span>
    <span class="n">tmp0</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">in_ptr0</span> <span class="o">+</span> <span class="p">(</span><span class="n">r0_1</span> <span class="o">+</span> <span class="mi">1000</span><span class="o">*</span><span class="n">x0</span><span class="p">),</span> <span class="n">r0_mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">tmp1</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">in_ptr1</span> <span class="o">+</span> <span class="p">(</span><span class="n">r0_1</span> <span class="o">+</span> <span class="mi">1000</span><span class="o">*</span><span class="n">x0</span><span class="p">),</span> <span class="n">r0_mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">tmp2</span> <span class="o">=</span> <span class="n">tmp0</span> <span class="o">+</span> <span class="n">tmp1</span>
    <span class="n">tmp3</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">tmp4</span> <span class="o">=</span> <span class="n">tmp2</span> <span class="o">*</span> <span class="n">tmp3</span>
    <span class="n">tmp5</span> <span class="o">=</span> <span class="n">tmp4</span> <span class="o">+</span> <span class="n">tmp0</span>
    <span class="n">tmp6</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">full</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="n">tl</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">tmp7</span> <span class="o">=</span> <span class="n">triton_helpers</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">tmp6</span><span class="p">,</span> <span class="n">tmp5</span><span class="p">)</span>
    <span class="n">tmp8</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">tmp7</span><span class="p">)</span>
    <span class="n">tmp9</span> <span class="o">=</span> <span class="n">tmp7</span> <span class="o">*</span> <span class="n">tmp8</span>
    <span class="n">tmp10</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">broadcast_to</span><span class="p">(</span><span class="n">tmp9</span><span class="p">,</span> <span class="p">[</span><span class="n">R0_BLOCK</span><span class="p">])</span>
    <span class="n">tmp12</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">r0_mask</span><span class="p">,</span> <span class="n">tmp10</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">tmp13</span> <span class="o">=</span> <span class="n">triton_helpers</span><span class="p">.</span><span class="nf">promote_to_tensor</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">tmp12</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">tl</span><span class="p">.</span><span class="nf">store</span><span class="p">(</span><span class="n">out_ptr0</span> <span class="o">+</span> <span class="p">(</span><span class="n">x0</span><span class="p">),</span> <span class="n">tmp13</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="sh">'''</span><span class="s">, device_str=</span><span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span><span class="s">)
</span></pre></td></tr></tbody></table></code></div></div>
<h2 id="-key-takeaways"><span class="me-2">ðŸŽ“ Key Takeaways</span><a href="#-key-takeaways" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<h3 id="what-youve-learned"><span class="me-2">What Youâ€™ve Learned</span><a href="#what-youve-learned" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>

<ol>
  <li><strong>TorchDynamo</strong>: Converts Python functions to computational graphs through sophisticated bytecode analysis and symbolic execution</li>
  <li><strong>TorchInductor</strong>: Optimizes graphs and generates backend-specific code (Triton)</li>
</ol>

<h2 id="next-steps"><span class="me-2">Next Steps</span><a href="#next-steps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2>

<p>In the next series, we will:</p>

<h3 id="deep-dive-into-complicated-cases"><span class="me-2">Deep Dive into Complicated Cases:</span><a href="#deep-dive-into-complicated-cases" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>We will deep dive further into some common case handled by torch compile when we are running model training, including but not limited to:</p>
<ul>
  <li>AOT Autograd</li>
  <li>torch.compile and backward pass</li>
  <li>torch.comiple and Distributed Data Parallel (DDP)</li>
  <li>torch.compile and Fully Sharded Distributed Parallel (FSDP2)</li>
</ul>

<h3 id="practical-debugging-guide"><span class="me-2">Practical Debugging Guide</span><a href="#practical-debugging-guide" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>We will share more practical debugging tools and variables when dealing with common issues.</p>

<h3 id="from-triton-kernel-to-gpu-binary"><span class="me-2">From Triton kernel to GPU binary</span><a href="#from-triton-kernel-to-gpu-binary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3>
<p>What happens next after Triton Python DSL? We will have further compliation to optimize specifically for certain hardware (ex. tritonIR -&gt; LLVM IR -&gt; PTX -&gt; cubin). We will introduce these in the following sections as well!</p>


  </div>

  <div class="post-tail-wrapper text-muted">
    <!-- categories -->
    
      <div class="post-meta mb-3">
        <i class="far fa-folder-open fa-fw me-1"></i>
        
          <a href="/categories/machine-learning/">Machine Learning</a>,
          <a href="/categories/pytorch/">PyTorch</a>
      </div>
    

    <!-- tags -->
    
      <div class="post-tags">
        <i class="fa fa-tags fa-fw me-1"></i>
        
          <a
            href="/tags/pytorch/"
            class="post-tag no-text-decoration"
          >pytorch</a>
        
          <a
            href="/tags/torch-compile/"
            class="post-tag no-text-decoration"
          >torch.compile</a>
        
          <a
            href="/tags/triton/"
            class="post-tag no-text-decoration"
          >triton</a>
        
          <a
            href="/tags/optimization/"
            class="post-tag no-text-decoration"
          >optimization</a>
        
          <a
            href="/tags/deep-learning/"
            class="post-tag no-text-decoration"
          >deep-learning</a>
        
      </div>
    

    <div
      class="
        post-tail-bottom
        d-flex justify-content-between align-items-center mt-5 pb-2
      "
    >
      <div class="license-wrapper">
        
          

          This post is licensed under 
        <a href="https://creativecommons.org/licenses/by/4.0/">
          CC BY 4.0
        </a>
         by the author.
        
      </div>

      <!-- Post sharing snippet -->

<div class="share-wrapper d-flex align-items-center">
  <span class="share-label text-muted">Share</span>
  <span class="share-icons">
    
    
    

    

      

      <a href="https://twitter.com/intent/tweet?text=Torch.compile%20101%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache&url=https%3A%2F%2Fsupercharles.github.io%2Fposts%2F2025-08-06-00%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter">
        <i class="fa-fw fa-brands fa-square-x-twitter"></i>
      </a>
    

      

      <a href="https://www.facebook.com/sharer/sharer.php?title=Torch.compile%20101%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache&u=https%3A%2F%2Fsupercharles.github.io%2Fposts%2F2025-08-06-00%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook">
        <i class="fa-fw fab fa-facebook-square"></i>
      </a>
    

      

      <a href="https://t.me/share/url?url=https%3A%2F%2Fsupercharles.github.io%2Fposts%2F2025-08-06-00%2F&text=Torch.compile%20101%20-%20From%20Python%20Function%20to%20Triton%20Kernel%20-%20ZCache" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram">
        <i class="fa-fw fab fa-telegram"></i>
      </a>
    

    <button
      id="copy-link"
      aria-label="Copy link"
      class="btn small"
      data-bs-toggle="tooltip"
      data-bs-placement="top"
      title="Copy link"
      data-title-succeed="Link copied successfully!"
    >
      <i class="fa-fw fas fa-link pe-none fs-6"></i>
    </button>
  </span>
</div>

    </div>
    <!-- .post-tail-bottom -->
  </div>
  <!-- div.post-tail-wrapper -->
</article>


            
          </main>

          <!-- panel -->
          <aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted">
            <div class="access">
              <!-- Get 5 last posted/updated posts -->














  <section id="access-lastmod">
    <h2 class="panel-heading">Recently Updated</h2>
    <ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/2025-08-06-00/">Torch.compile 101 - From Python Function to Triton Kernel</a>
        </li>
      
        
        
        
        <li class="text-truncate lh-lg">
          <a href="/posts/2025-08-13-00/">From FlashAttention to FlashAttention 2</a>
        </li>
      
    </ul>
  </section>
  <!-- #access-lastmod -->


              <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/optimization/">optimization</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/torch-compile/">torch.compile</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/triton/">triton</a>
      
    </div>
  </section>


            </div>

            
              
              






  <div class="toc-border-cover z-3"></div>
  <section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4">
    <h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2>
    <nav id="toc"></nav>
  </section>


            
          </aside>
        </div>

        <div class="row">
          <!-- tail -->
          <div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
            
              
              <!-- Recommend the other 3 posts according to the tags and categories of the current post. -->

<!-- The total size of related posts -->


<!-- An random integer that bigger than 0 -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy} -->















  

  
    
  

  

  

  

  

  











  <aside id="related-posts" aria-labelledby="related-label">
    <h3 class="mb-4" id="related-label">Further Reading</h3>
    <nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4">
      
        <article class="col">
          <a href="/posts/2025-08-13-00/" class="post-preview card h-100">
            <div class="card-body">
              <!--
  Date format snippet
  See: ${JS_ROOT}/utils/locale-dateime.js
-->




<time
  
  data-ts="1755136800"
  data-df="ll"
  
>
  Aug 13, 2025
</time>

              <h4 class="pt-0 my-2">From FlashAttention to FlashAttention 2</h4>
              <div class="text-muted">
                <p>Deep dive into FlashAttention 2</p>
              </div>
            </div>
          </a>
        </article>
      
    </nav>
  </aside>
  <!-- #related-posts -->


            
              
              <!-- Navigation buttons at the bottom of the post. -->

<nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation">
  
  

  
    <div class="btn btn-outline-primary disabled" aria-label="Older">
      <p>-</p>
    </div>
  

  
    <a
      href="/posts/2025-08-13-00/"
      class="btn btn-outline-primary"
      aria-label="Newer"
    >
      <p>From FlashAttention to FlashAttention 2</p>
    </a>
  
</nav>

            

            <!-- The Footer -->

<footer
  aria-label="Site Info"
  class="
    d-flex flex-column justify-content-center text-muted
    flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3
  "
>
  <p>Â©
    <time>2025</time>

    
      <a href="https://twitter.com/supercharles">supercharles</a>.
    

    
      <span
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."
      >Some rights reserved.</span>
    
  </p>

  <p>Using the <a
        data-bs-toggle="tooltip"
        data-bs-placement="top"
        title="v7.3.1"
        href="https://github.com/cotes2020/jekyll-theme-chirpy"
        target="_blank"
        rel="noopener"
      >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.
  </p>
</footer>

          </div>
        </div>

        <!-- The Search results -->

<div id="search-result-wrapper" class="d-flex justify-content-center d-none">
  <div class="col-11 content">
    <div id="search-hints">
      <!-- The trending tags list -->















  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        

  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
    
  
    
    
    
    
      
        
        



  <section>
    <h2 class="panel-heading">Trending Tags</h2>
    <div class="d-flex flex-wrap mt-3 mb-1 me-3">
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/deep-learning/">deep-learning</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/optimization/">optimization</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/pytorch/">pytorch</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/torch-compile/">torch.compile</a>
      
        
        <a class="post-tag btn btn-outline-primary" href="/tags/triton/">triton</a>
      
    </div>
  </section>


    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>

      </div>

      <aside aria-label="Scroll to Top">
        <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow">
          <i class="fas fa-angle-up"></i>
        </button>
      </aside>
    </div>

    <div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div>

    
      <aside
  id="notification"
  class="toast"
  role="alert"
  aria-live="assertive"
  aria-atomic="true"
  data-bs-animation="true"
  data-bs-autohide="false"
>
  <div class="toast-header">
    <button
      type="button"
      class="btn-close ms-auto"
      data-bs-dismiss="toast"
      aria-label="Close"
    ></button>
  </div>
  <div class="toast-body text-center pt-0">
    <p class="px-2 mb-3">A new version of content is available.</p>
    <button type="button" class="btn btn-primary" aria-label="Update">
      Update
    </button>
  </div>
</aside>

    

    <!-- Embedded scripts -->

    
      
      <!-- The comments switcher -->


    

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script>
  
  document.addEventListener('DOMContentLoaded', () => {
    SimpleJekyllSearch({
      searchInput: document.getElementById('search-input'),
      resultsContainer: document.getElementById('search-results'),
      json: '/assets/js/data/search.json',
      searchResultTemplate: '  <article class="px-1 px-sm-2 px-lg-4 px-xl-0">    <header>      <h2><a href="{url}">{title}</a></h2>      <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">        {categories}        {tags}      </div>    </header>    <p>{content}</p>  </article>',
      noResultsText: '<p class="mt-5">Oops! No results found.</p>',
      templateMiddleware: function(prop, value, template) {
        if (prop === 'categories') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
          }
        }

        if (prop === 'tags') {
          if (value === '') {
            return `${value}`;
          } else {
            return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
          }
        }
      }
    });
  });
</script>

  </body>
</html>

